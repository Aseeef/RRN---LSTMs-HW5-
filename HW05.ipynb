{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 505 Homework 05:  Recurrent Neural Networks\n",
    "\n",
    "#### Due Friday  11/17 at midnight (1 minute after 11:59 pm) in Gradescope (with a grace period of 6 hours)\n",
    "#### You may submit the homework up to 24 hours late (with the same grace period) for a penalty of 10%. \n",
    "\n",
    "All homeworks will be scored with a maximum of 100 points; point values are given\n",
    "for individual problems, and if parts of problems do not have point values given, they\n",
    "will be counted equally toward the total for that problem. \n",
    "\n",
    "Note: This homework is a bit different from the first four in this class in that in some parts we are specified **what** you need to do for your solutions, but much less of the **how** you write the details of the code. There are three reasons for this:\n",
    "\n",
    "- In a graduate level CS class, after four homeworks and two months of lectures, you should be well-equipped to work out the coding issues for yourself, and in general, going forward, this is how you will solve the kinds of problems presented here; \n",
    "- Suggestions for resources (mostly ML blogs) will be suggested; there are many resources, but these are from bloggers that I trust and have used in the past;\n",
    "- I am expecting that you will make good use of chatGPT for help with the details of syntax and low-level organization of your code. There is often nothing very stimulating or informative about precisely what is the syntax needed for a particular kind of layer in a network, and rather than poke around on StackOverflow, chatGPT is particularly good at summarizing existing approaches to ML coding tasks. \n",
    "\n",
    "#### Submission Instructions\n",
    "\n",
    "You must complete the homework by editing <b>this notebook</b> and submitting the following two files in Gradescope by the due date and time:\n",
    "\n",
    "  - A file <code>HW05.ipynb</code> (be sure to select <code>Kernel -> Restart and Run All</code> before you submit, to make sure everything works); and\n",
    "  - A file <code>HW05.pdf</code> created from the previous.\n",
    "  \n",
    "  For best results obtaining a clean PDF file on the Mac, select <code>File -> Print Review</code> from the Jupyter window, then choose <code>File-> Print</code> in your browser and then <code>Save as PDF</code>.  Something  similar should be possible on a Windows machine -- just make sure it is readable and no cell contents have been cut off. Make it easy to grade!\n",
    "  \n",
    "The date and time of your submission is the last file you submitted, so if your IPYNB file is submitted on time, but your PDF is late, then your submission is late. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborators (5 pts)\n",
    "\n",
    "Describe briefly but precisely\n",
    "\n",
    "1. Any persons you discussed this homework with and the nature of the discussion;\n",
    "2. Any online resources you consulted and what information you got from those resources; and\n",
    "3. Any AI agents (such as chatGPT or CoPilot) or other applications you used to complete the homework, and the nature of the help you received. \n",
    "\n",
    "A few brief sentences is all that I am looking for here. \n",
    "\n",
    "    <Your answer here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T00:45:44.676430300Z",
     "start_time": "2023-11-23T00:45:11.201812200Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import shuffle, seed, choice\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split,Dataset,DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, brown\n",
    "import pickle\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T00:45:44.685427400Z",
     "start_time": "2023-11-23T00:45:44.675429Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing import Dict\n",
    "from typing import Any\n",
    "from typing import Union\n",
    "from typing import DefaultDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T00:45:44.694426700Z",
     "start_time": "2023-11-23T00:45:44.689427900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell contains some helpful methods to dump notebook variables\n",
    "to a file so you don't have to rerun expensive computations every\n",
    "time.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "def does_var_exists(var_name: str) -> bool:\n",
    "    return os.path.isfile(F'./data/pickle/{var_name}.pkl')\n",
    "\n",
    "def dump_var(var_name: str, obj) -> None:\n",
    "    with open(F'./data/pickle/{var_name}.pkl', 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "\n",
    "def load_var(var_name: str) -> Union[None, object]:\n",
    "    if not does_var_exists(var_name):\n",
    "        return None\n",
    "    with open(F'./data/pickle/{var_name}.pkl', 'rb') as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T00:45:44.715446700Z",
     "start_time": "2023-11-23T00:45:44.696443400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dirs where I'll be storing my data\n",
    "os.makedirs('./data/glove', exist_ok=True)\n",
    "os.makedirs('./data/java', exist_ok=True)\n",
    "os.makedirs('./data/pickle', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem One:  Character-Level Generative Model (20 pts)\n",
    "\n",
    "A basic character-level model has been provided on the class web site in the row for Lecture 14: \n",
    "<a href=\"https://www.cs.bu.edu/fac/snyder/cs505/CharacterLevelLSTM.ipynb\">IPYNB</a>. Your first step is to download this and run it in Colab (or download the data file, which is in the CS 505 Data Directory and also linked on the web site, and run it on your local machine) and understand all its various features. Most of it is straight-forward at this point in the course, but the definition of the model is a bit messy, and you will need to read about LSTM layers in the Pytorch documents to really understand what it is doing and what the hyperparameters mean. \n",
    "\n",
    "Also take a look at the article \"The Unreasonable Effectiveness of Recurrent Neural Networks\" linked with lecture 14. \n",
    "\n",
    "For this problem, you will run this code on a dataset consisting of Java code files, which has been uploaded to the CS 505 Data Directory and also to the class web site: <a href=\"https://www.cs.bu.edu/fac/snyder/cs505/JavaFiles/\">DIR</a>  Select some number of these files and concatenate them into one long text file, such that you have approximately 10-20K characters (if you have trouble running out of RAM you can use fewer, but try to get at least 10K). \n",
    "\n",
    "You will run the character-level model on this dataset. You may either cut and paste code into this notebook, or submit the file with your changes and output along with this notebook to Gradescope.\n",
    "\n",
    "Your task is to get a character-level model that has not simply memorized the Java text file by overfitting, and does not do much other than spit out random characters (underfitting).  You will get the former if you simply run it for many epochs without any changes to the hyperparameters; you will get the latter if you run it only a few epochs. \n",
    "\n",
    "You should experiment with different hyperparameters, which in the notebook are indicated\n",
    "by \n",
    "\n",
    "          <== something to play with\n",
    "\n",
    "and try to get a model that seems to recognize typical Java syntax such as comments, matching parentheses, expressions, assignments, and formatting, but is not just repeating\n",
    "exact text from the data file. Clearly, the number of epochs plays a crucial role, but I also want you to\n",
    "experiment with the various hyperparameters to try to avoid overfitting. See my lectures on T 10/31 and Th 11/2 (recorded and on my YT channel) for the background to this.\n",
    "\n",
    "Note that the code you will work from does not use validation and testing sets, nor does it calculate the accuracy, but only tracks the loss. The nature of the data sets for character-level models does not seem to lend itself to accuracy metrics, but you may wish to try this -- I have not found it to be useful, but have simply focussed on the output and \"eyeballed\" the results to determine how much they have generalized\n",
    "from the data. \n",
    "\n",
    "Submit your notebook(s) to Gradescope as usual, and also provide a summary of your results in the next cell. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:13.967754600Z",
     "start_time": "2023-11-22T03:12:13.916415100Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the java sources\n",
    "java_source_list = []\n",
    "num_chars = 0\n",
    "\n",
    "files = os.listdir('./data/java/')\n",
    "random.shuffle(files)\n",
    "for java_file in files:\n",
    "    # only add full files (because I don't want training code that doesn't have closing brackets\n",
    "    with open('./data/java/' + java_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        num_chars += len('\\n'.join(lines))\n",
    "        # don't go over 32k characters (note: I deviate a bit from the \"10k to 20k\" suggestion)\n",
    "        if num_chars > 32000:\n",
    "            break\n",
    "        java_source_list += lines\n",
    "\n",
    "java_source_text = '\\n'.join(java_source_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:14.722643100Z",
     "start_time": "2023-11-22T03:12:14.704643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is 31509 characters long.\n",
      "First 200 characters: /* File: RecursiveGraphics.java\n",
      "\n",
      " * Author: \n",
      "\n",
      " * Date: \n",
      "\n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      "\n",
      " */\n",
      "\n",
      "\n",
      "\n",
      "import java.awt.Color;\n",
      "\n",
      "import java.awt.Canvas;\n",
      "\n",
      "import java .awt.Graphics;\n",
      "\n",
      "import\n"
     ]
    }
   ],
   "source": [
    "print(f\"Text is {len(java_source_text)} characters long.\")\n",
    "print(f\"First 200 characters: {java_source_text[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:18.062743Z",
     "start_time": "2023-11-22T03:12:18.045604500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# do some cleaning\n",
    "cleaned_java_source_text = java_source_text\n",
    "# remove double new lines\n",
    "cleaned_java_source_text = re.sub(\"\\n\\n\", \"\\n\", cleaned_java_source_text)\n",
    "# remove lines with only whitespace\n",
    "cleaned_java_source_text = re.sub(r'^\\s*$', \"\", cleaned_java_source_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:18.636454900Z",
     "start_time": "2023-11-22T03:12:18.623476100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 200 characters (cleaned): /* File: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "import java .awt.Graphics;\n",
      "import javax.sw\n"
     ]
    }
   ],
   "source": [
    "print(f\"First 200 characters (cleaned): {cleaned_java_source_text[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:19.167158Z",
     "start_time": "2023-11-22T03:12:19.038641400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 89 unique characters in the text.\n",
      "Character set: ['\\t', '\\n', ' ', '!', '\"', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}'].\n"
     ]
    }
   ],
   "source": [
    "chars_in_text = sorted(list(set(cleaned_java_source_text)))\n",
    "num_chars = len(chars_in_text)\n",
    "\n",
    "print(f'There are {num_chars} unique characters in the text.')\n",
    "print(f'Character set: {chars_in_text}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:19.621073200Z",
     "start_time": "2023-11-22T03:12:19.609877300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create functions mapping characters to integers and back\n",
    "\n",
    "def char2int(c):\n",
    "    return chars_in_text.index(c)\n",
    "\n",
    "def int2char(i):\n",
    "    return chars_in_text[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:27.863164900Z",
     "start_time": "2023-11-22T03:12:27.847164700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HYPER PARAMETERS HERE\n",
    "sample_len = 160 # <== something to play with\n",
    "batch_size = 512 # <== something to play with\n",
    "model_dropout= 0.4 # <== something to play with\n",
    "hidden_dim_size = 316 # <== something to play with\n",
    "n_layers_count = 3 # <== something to play with\n",
    "leaning_rate = 0.001 # <== something to play with\n",
    "weight_decay = 0.00015 # <== something to play with\n",
    "model_temp = 0.7 # <== something to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:28.551460500Z",
     "start_time": "2023-11-22T03:12:28.521724300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence:\n",
      "/* File: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "i\n",
      "Target sequence:\n",
      "* File: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "im\n",
      "\n",
      "Input sequence:\n",
      "* File: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "im\n",
      "Target sequence:\n",
      " File: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "imp\n",
      "\n",
      "Input sequence:\n",
      " File: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "imp\n",
      "Target sequence:\n",
      "File: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "impo\n",
      "\n",
      "Input sequence:\n",
      "File: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "impo\n",
      "Target sequence:\n",
      "ile: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "impor\n",
      "\n",
      "Input sequence:\n",
      "ile: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "impor\n",
      "Target sequence:\n",
      "le: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "import\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating lists that will hold our input and target sample sequences\n",
    "\n",
    "input_seq_chars = []\n",
    "target_seq_chars = []\n",
    "\n",
    "for k in range(len(cleaned_java_source_text)-sample_len+1):\n",
    "\n",
    "    # Remove last character for input sequence\n",
    "    input_seq_chars.append(cleaned_java_source_text[k:k+sample_len-1])\n",
    "\n",
    "    # Remove firsts character for target sequence\n",
    "    target_seq_chars.append(cleaned_java_source_text[k+1:k+sample_len])\n",
    "\n",
    "for i in range(5):\n",
    "    print(f'Input sequence:\\n{input_seq_chars[i]}')\n",
    "    print(f'Target sequence:\\n{target_seq_chars[i]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:29.412658900Z",
     "start_time": "2023-11-22T03:12:29.398628100Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert an integer into a one-hot encoding of the given size (= number of characters)\n",
    "def int2OneHot(X,size):\n",
    "\n",
    "    def int2OneHot1(x,size=10):\n",
    "        tmp = np.zeros(size)\n",
    "        tmp[int(x)] = 1.0\n",
    "        return tmp\n",
    "\n",
    "    return np.array([ int2OneHot1(x, size) for x in X ]).astype('double')\n",
    "\n",
    "# do the same thing, but for a list/array of integers\n",
    "def seq2OneHot(seq,size):\n",
    "    return np.array([ int2OneHot(x, size) for x in seq ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:42.269289600Z",
     "start_time": "2023-11-22T03:12:30.094474100Z"
    }
   },
   "outputs": [],
   "source": [
    "input_seq = []\n",
    "for i in range(len(input_seq_chars)):\n",
    "    input_seq.append( [char2int(ch) for ch in input_seq_chars[i]])\n",
    "input_seq = seq2OneHot(input_seq,size=num_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:54.400574800Z",
     "start_time": "2023-11-22T03:12:42.290470100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_seq = []\n",
    "for i in range(len(input_seq_chars)):\n",
    "    target_seq.append([char2int(ch) for ch in target_seq_chars[i]])\n",
    "target_seq = seq2OneHot(target_seq,size=num_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:54.409578600Z",
     "start_time": "2023-11-22T03:12:54.399573300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: (30480, 159, 89)\n",
      "target shape: (30480, 159, 89)\n"
     ]
    }
   ],
   "source": [
    "print('input shape:', input_seq.shape)\n",
    "print('target shape:', target_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:57.392332Z",
     "start_time": "2023-11-22T03:12:54.407597200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_seq = torch.Tensor(input_seq).type(torch.DoubleTensor)\n",
    "target_seq = torch.Tensor(target_seq).type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:57.405332200Z",
     "start_time": "2023-11-22T03:12:57.396331200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30480"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Basic_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # return a pair x,y at the index idx in the data set\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "ds = Basic_Dataset(input_seq,target_seq)\n",
    "ds.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:57.454515600Z",
     "start_time": "2023-11-22T03:12:57.419568500Z"
    }
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:57.465516600Z",
     "start_time": "2023-11-22T03:12:57.427154700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:57.480663900Z",
     "start_time": "2023-11-22T03:12:57.454515600Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, dropout):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers,dropout=dropout,batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        hidden_state_size = x.size(0)\n",
    "\n",
    "        x = x.to(torch.double)\n",
    "\n",
    "        h0 = torch.zeros(self.n_layers,hidden_state_size,self.hidden_dim).double().to(device)\n",
    "        c0 = torch.zeros(self.n_layers,hidden_state_size,self.hidden_dim).double().to(device)\n",
    "\n",
    "        self.lstm = self.lstm.double()\n",
    "\n",
    "        self.fc1 = self.fc1.double()\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, (hx,cx) = self.lstm(x, (h0,c0))\n",
    "\n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:57.483659200Z",
     "start_time": "2023-11-22T03:12:57.480663900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (lstm): LSTM(89, 316, num_layers=3, batch_first=True, dropout=0.4)\n",
      "  (fc1): Linear(in_features=316, out_features=89, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "\n",
    "model = Model(input_size=num_chars, output_size=num_chars, hidden_dim=hidden_dim_size, n_layers=n_layers_count,dropout=model_dropout)\n",
    "\n",
    "print(model)\n",
    "\n",
    "model = model.double().to(device)\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=leaning_rate,weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:57.532379700Z",
     "start_time": "2023-11-22T03:12:57.480663900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:24:33.755824Z",
     "start_time": "2023-11-22T03:22:58.652442300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:34<00:00, 23.74s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "    for input_seq_batch,target_seq_batch in data_loader:\n",
    "        input_seq_batch = input_seq_batch.to(device)\n",
    "        target_seq_batch = target_seq_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        target_seq_hat = model(input_seq_batch)\n",
    "        loss = loss_fn(target_seq_hat,target_seq_batch.view(-1,num_chars))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:24:38.801343Z",
     "start_time": "2023-11-22T03:24:37.856672200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14d55d511a58>]"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:48:00.682335800Z",
     "start_time": "2023-11-22T03:48:00.538733800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#file_path = './data/pickle/final_model.torch'\n",
    "#torch.save(model, file_path)\n",
    "#dump_var(\"losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:24:43.140354800Z",
     "start_time": "2023-11-22T03:24:42.997791600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of softmax with temperature.\n",
      "distribution: [0.1, 0.3, 0.6]\n",
      "[1.9287498479637375e-22, 9.3576229688393e-14, 0.9999999999999064]\n",
      "[0.006377460922442302, 0.04712341652466416, 0.9464991225528936]\n",
      "[0.06289001324586753, 0.1709527801977903, 0.7661572065563421]\n",
      "[0.12132647558421489, 0.23631170657656433, 0.6423618178392208]\n",
      "[0.2583896517379799, 0.3155978333128144, 0.4260125149492058]\n",
      "[0.3255767455856355, 0.3321538321280155, 0.3422694222863489]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def softmax_with_temperature(vec, temperature):\n",
    "    sum_exp = sum(math.exp(x/temperature) for x in vec)\n",
    "    return [math.exp(x/temperature)/sum_exp for x in vec]\n",
    "\n",
    "print(\"Example of softmax with temperature.\")\n",
    "dist = [0.1, 0.3, 0.6]\n",
    "print('distribution:',dist)\n",
    "print(softmax_with_temperature(dist,0.01))\n",
    "print(softmax_with_temperature(dist,0.1))\n",
    "print(softmax_with_temperature(dist,0.2))\n",
    "print(softmax_with_temperature(dist,0.3))\n",
    "print(softmax_with_temperature(dist,1))\n",
    "print(softmax_with_temperature(dist,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:24:44.182602700Z",
     "start_time": "2023-11-22T03:24:43.915040800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature = model_temp\n",
    "\n",
    "def predict(model, ch):\n",
    "\n",
    "    # only look at last sample_len - 1 characters\n",
    "\n",
    "    ch = ch[-(sample_len - 1):]\n",
    "\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    ch = np.array([char2int(c) for c in ch])\n",
    "    ch = np.array([int2OneHot(ch, num_chars)])\n",
    "    ch = torch.from_numpy(ch).to(device)\n",
    "\n",
    "    out = model(ch)\n",
    "\n",
    "    # take the probability distribution of the last character in the sequence produced by the model\n",
    "    prob = softmax_with_temperature(out[-1],temperature)\n",
    "\n",
    "    # Choosing a character based on the probability distribution, with temperature\n",
    "    char_ind = choice(list(range(num_chars)), p=prob)\n",
    "\n",
    "    return int2char(char_ind)\n",
    "\n",
    "predict(model,\"public static \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:24:46.819524500Z",
     "start_time": "2023-11-22T03:24:46.817521400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample(model, out_len, start):\n",
    "    model.eval() # eval mode\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size):\n",
    "        char = predict(model, chars)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:25:18.549852300Z",
     "start_time": "2023-11-22T03:24:46.819524500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public static void drawSpiralHelper(double x, double y, double s, boolean horizontal, int depth, Graphics g) {\n",
      " \t\n",
      "\tif(depth == 0) {\n",
      "           return;\n",
      "       }\n",
      "       \n",
      "       // draw lines\n",
      "       \n",
      "       double x1 = x0 + len * Math.cos(angle);\n",
      "       double y1 = y0 - len * Math.sin(angle);\n",
      " \n",
      "       drawLine(x0,y0,x1,y1,g);\n",
      "       \n",
      "       for (int i = 0; i < rDelta.length; ++i) {\n",
      "          changeColorByRecursionDepth(depth); \n",
      "          drawTreeHelper(x1, y1, len * lenDelta[i], angle + thetaDelta[i], depth-1,g);\n",
      "       }\n",
      "   }\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    //  Problem 6.1    Your Turn!\n",
      "    //  For this one, you have to figure aut how to do the recursive cales on tem .\n",
      "    public void drawLine(double x1, double y1, double x5, double y5, int depth, Graphics g) {\n",
      "       if (depth == 2) {\n",
      "\t    drawLine(x1, y1, x5, y5, g);\n",
      "       }\n",
      "       else {\n",
      "           \n",
      "           double x2, x3, x4;\n",
      "           double y2, y1, y3, y5;  \n",
      "            \n",
      "           double deltaX = x5 - x1;\n",
      "           double deltaY = y5 - x1;\n",
      "           double deltaY = y5 - y1;\n",
      "           \n",
      "           x2 = x1 + deltaX/3.0;\n",
      "           y2 = y1 + deltaY/3.0;\n",
      "           x4 = y5 - deltaY/3.0;\n",
      "           x3 = ((x1 + x5)/2.0 - (Math.sqrt(3.0)/6.0) * (y5 - y5));\n",
      "           double y3 = ((y1 + y5)/2.0 + (Math.sqrt(3.0)/6.0) * (y5 - y5));\n",
      "           y3 = ((y1 + y5)/2 + ( (Math.sqrt(3.0)/6.0) * (y5 - y5));\n",
      "\t   \n",
      "           double x3 = ((x1 + x5)/2.0 - (Math.sqrt(3.0)/6.0) * (y1 - y1));\n",
      "           x3 = ((x1 + x5)/2.0 + (Math.sqrt(3.0)/6.0) * (x1 - y1));\n",
      "           x3 = (y1 + deltaX/3.0;\n",
      "           x2 =  ( x1 + x) --  Math.sqrt(3.0);\n",
      "        double y[] = y    Math.sqrt(3; \n",
      "        double[] ys = new double[3];\n",
      "        xs[0] = x; \n",
      "        ys[0] = y;\n",
      "        ys[1] = x-s/2;\n",
      "        ys[2] = y+s/2;\n",
      "        ys[2] = y+s*0.66;\n",
      "        x1 = y + Meth.sqrt(atth).0.5 * Math.cIs40;\n",
      "        if(depth == 0) {\n",
      "           return;\n",
      "       }\n",
      "       \n",
      "       // draw lines\n",
      "       \n",
      "       double x1 = x0 - len * Math.cos(angle);\n",
      "       double y1 = y0 - len * Math.sin(angle);\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, 2000, \"public static \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnB5z-ZoZEa5"
   },
   "source": [
    "### Your analysis\n",
    "\n",
    "Please describe your experiments and cut and paste various outputs to show how the model performed at\n",
    "various numbers of epochs and with various hyperparameters. What characteristics of Java was it able to learn? What did it not learn? The article \"The Unreasonable ...\" does a nice job of showing this kind of behavior as the number of epochs increases, and you might look at it before writing your answer here. \n",
    "\n",
    "My goal going into this was to create a model that understands Java syntax (including at the very least, opening and closing brackets) as stated in the problem.\n",
    "\n",
    "Starting with the example code in the CharacterLevelLSTM.ipynb, my first attempts to get an effective model were disappointing. With my initial hyperparameters copied from the CharacterLevelLSTM.ipynb, my model was memorizing things way too quick (in less than 20 epochs). Here were some results from 50 epochs:\n",
    "Code Generations: https://share.aseef.dev/bDfMCkP\n",
    "Losses: https://share.aseef.dev/WHcLkjc\n",
    "\n",
    "In order to prevent memorization I played around with several things:\n",
    "a) Used drop-out rates from 30%-50% to prevent memorization\n",
    "b) Played around with layers low as 1 and deep as 6 in hopes the model can better extract abstract info\n",
    "c) Increased batch size (though this was just to train faster as my model size grew)\n",
    "d) Reduced the number of feature states by half in hopes that the model won't have enough states to memorize the data (from 96-256) - but this didn't work well in practice.\n",
    "e) Also played around with the sample lengths and found though higher sample lengths took longer to train, they produced higher quality models\n",
    "\n",
    "In this experimentation, the challenges I encountered were that the dropout rates were simply too high on one spectrum and the model just stopped learning after one point (no matter how many more epochs I would train it for):\n",
    "Code Generation: https://share.aseef.dev/coGIakH\n",
    "Losses: https://share.aseef.dev/Er2mKLm\n",
    "\n",
    "But simply just reducing dropout rates didn't help much either (with a dropout rate of 0.3):\n",
    "Code Generation: https://share.aseef.dev/LMdsJCE\n",
    "Losses: https://share.aseef.dev/rHDRPH7\n",
    "\n",
    "In the end, nothing worked and I was not able to produce any \"decent\" models. My models were either garbage, or memorized. That was until I took a page from \"the unreasonable effectiveness of data\" and simply added about 50% more data by increasing my data size to up to 32k characters!\n",
    "\n",
    "My final model finally produces outputs that aren't fully memorized and finally starts to demonstrate some understanding of brackets, indenting (mostly), methods, function calls and etc. In fact, the model went beyond my expectation and the code it wrote was still relevant to the name of the method (e.i. wasn't making just completely random calls).\n",
    "Code Generation: https://share.aseef.dev/i9WE5Tg\n",
    "Losses: https://share.aseef.dev/NoAtm44\n",
    "\n",
    "I think the final drop out rates I ended up using (at 40%) was also an important choice that helped the model finally generalize to a satisfactory model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Two:  Word-Level Generative Model (40 pts)\n",
    "\n",
    "In this problem you will write another generative model, as you did in HW 03, but this time you will use an LSTM network, GloVe word embeddings, and beam search. \n",
    "\n",
    "Before you start, read the following blog post to see the core ideas involved in creating a generative model using word embeddings:\n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/\n",
    "\n",
    "You may also wish to consult with chatGPT about how to develop this kind of model in Pytorch.\n",
    "\n",
    "The requirements for this problem are as follows (they mostly consist of the extensions proposed at\n",
    "the end of the blog post linked above):\n",
    "\n",
    "- Develop your code in Pytorch, not Keras\n",
    "- Use the novel *Persuation* by Jane Austen as your training data (available through the Brown Corpus); if you have trouble with RAM you will need to cut down the number of sentences (perhaps by eliminating the longest sentences as well, see next point). \n",
    "- Develop a sentence-level model by padding sentences to the maximum sentence length in the novel (if this seems extreme, you may wish to delete a small number of the longest sentences to reduce the maximum length). Surround your data sentences with `<s>` and `</s>` and your model should generate one sentence at a time (as you did in HW 03), i.e., it should stop if it generates the `</s>` token. \n",
    "- Use pretrained GLoVe embeddings with dimension 200, and update them (refine by training further) on the sentences in the novel; if you have trouble with RAM you may use a smaller dimension. \n",
    "- Experiment with the hyperparameters (sample length, number of layers, uni- or bi-directional, weight_decay, dropout, number of epochs, temperature of the softmax, etc.) as you did in Problem One to find the \"sweet spot\" where you are generating interesting-looking sentences but not simply repeating sentences from the data. You may want to try adding more linear layers on top to pick the most likely next word. \n",
    "- Generate sentences using Beam Search, which we describe below. \n",
    "\n",
    "Your solution should be the code, samples of sentences generated with their score (described below), and your description of the investigation of various hyperparameters, and what strategy ended up seeming to generate the most realistic sentences that were not simply a repeat of sentences in the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T00:45:57.527589900Z",
     "start_time": "2023-11-23T00:45:44.705428Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /usr4/cs505ws/aseef/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#if does_var_exists('persuasion_doc'):\n",
    "#    persuasion_doc = load_var('persuasion_doc')\n",
    "#else:\n",
    "# download it from gutenberg\n",
    "nltk.download('gutenberg')\n",
    "# Load the text of Persuasion by Jane Austen\n",
    "persuasion_raw_text = gutenberg.raw('austen-persuasion.txt')\n",
    "# Tokenize the text into sentences\n",
    "#spacy.require_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "persuasion_doc = nlp(persuasion_raw_text)\n",
    "dump_var('persuasion_doc', persuasion_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T00:45:57.547605200Z",
     "start_time": "2023-11-23T00:45:57.537592800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The novel is 466292 characters long with 53 uncased unique characters {'2', 's', '5', 'y', 't', 'a', '-', '7', ']', 'o', ')', '`', '\"', '?', ' ', 'l', 'm', 'g', ';', \"'\", ',', ':', 'k', 'j', 'd', '.', 'w', '0', 'v', 'n', '4', 'r', 'z', '(', '8', '[', 'x', '&', '\\n', '9', 'u', '1', 'f', 'p', 'e', 'b', 'q', '6', '!', 'i', '3', 'c', 'h'}.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The novel is {len(persuasion_raw_text)} characters long with {len(set(persuasion_raw_text.lower()))} uncased unique characters {set(persuasion_raw_text.lower())}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T00:45:57.616126300Z",
     "start_time": "2023-11-23T00:45:57.548591400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert doc to a string list of sentences\n",
    "persuasion_sentences: List[List[str]] = []\n",
    "for sent in persuasion_doc.sents:\n",
    "    persuasion_sentences += [sent.__str__()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T00:45:57.643105700Z",
     "start_time": "2023-11-23T00:45:57.595105300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Persuasion by Jane Austen 1818]\n",
      "\n",
      "\n",
      "Chapter 1\n",
      "\n",
      "\n",
      "Sir Walter Elliot, of Kellynch Hall, in Somersetshire, was a man who,\n",
      "for his own amusement, never took up any book but the Baronetage;\n",
      "there he found occupation for an idle hour, and consolation in a\n",
      "distressed one; there his faculties were roused into admiration and\n",
      "respect, by contemplating the limited remnant of the earliest patents;\n",
      "there any unwelcome sensations, arising from domestic affairs\n",
      "changed naturally into pity and contempt as he turned over\n",
      "the almost endless creations of the last century; and there,\n",
      "if every other leaf were powerless, he could read his own history\n",
      "with an interest which never failed.  \n",
      "This was the page at which\n",
      "the favourite volume always opened:\n",
      "\n",
      "           \"ELLIOT OF KELLYNCH HALL.\n",
      "\n",
      "\n",
      "\"Walter Elliot, born March 1, 1760, married, July 15, 1784, Elizabeth,\n",
      "daughter of James Stevenson, Esq. of South Park, in the county of\n",
      "Gloucester, by which lady (who died 1800) he has issue Elizabeth,\n",
      "born June 1, 1785; Anne, born August 9, 1787; a still-born son,\n",
      "November 5, 1789; Mary, born November 20, 1791.\n"
     ]
    }
   ],
   "source": [
    "# print first 3 sentences to confirm stuff works\n",
    "for sent in persuasion_sentences[:3]:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T00:45:57.707637200Z",
     "start_time": "2023-11-23T00:45:57.599107700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> this was the page at which the favourite volume always opened elliot of kellynch hall </s>\n",
      "\n",
      "<s> walter elliot born march married july elizabeth daughter of james stevenson esq of south park in the county of gloucester by which lady who died he has issue elizabeth born june anne born august a stillborn son november mary born november </s>\n",
      "\n",
      "<s> precisely such had the paragraph originally stood from the printers hands but sir walter had improved it by adding for the information of himself and his family these words after the date of marys birth married december charles son and heir of charles musgrove esq of uppercross in the county of somerset and by inserting most accurately the day of the month on which he had lost his wife </s>\n",
      "\n",
      "<s> then followed the history and rise of the ancient and respectable family in the usual terms how it had been first settled in cheshire how mentioned in dugdale serving the office of high sheriff representing a borough in three successive parliaments exertions of loyalty and dignity of baronet in the first year of charles ii with all the marys and elizabeths they had married forming altogether two handsome duodecimo pages and concluding with the arms and mottoprincipal seat kellynch hall in the county of somerset and sir walters handwriting again in this finale heir presumptive william walter elliot esq great grandson of the second sir walter </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets clean up the sentences...\n",
    "persuasion_cleaned_sentences = persuasion_sentences\n",
    "# 1. get rid of the book title\n",
    "persuasion_cleaned_sentences: List[str] = persuasion_cleaned_sentences[1:]\n",
    "# 2. get rid of chapter titles\n",
    "for i in range(len(persuasion_cleaned_sentences)):\n",
    "    persuasion_cleaned_sentences[i] = re.sub(\"Chapter [0-9]+\", \"\", persuasion_cleaned_sentences[i])\n",
    "# 3. get rid of trailing whitespaces\n",
    "for i in range(len(persuasion_cleaned_sentences)):\n",
    "    persuasion_cleaned_sentences[i] = persuasion_cleaned_sentences[i].strip()\n",
    "# 4. lowercase everything\n",
    "for i in range(len(persuasion_cleaned_sentences)):\n",
    "    persuasion_cleaned_sentences[i] = persuasion_cleaned_sentences[i].lower()\n",
    "# 5. convert contractions\n",
    "for i in range(len(persuasion_cleaned_sentences)):\n",
    "    persuasion_cleaned_sentences[i] = persuasion_cleaned_sentences[i].replace(\"'ll\", \" will\")\n",
    "    persuasion_cleaned_sentences[i] = persuasion_cleaned_sentences[i].replace(\"that's\", \"that is\")\n",
    "    persuasion_cleaned_sentences[i] = persuasion_cleaned_sentences[i].replace(\"n't\", \" not\")\n",
    "# 6. handle punctuations\n",
    "# [TODO: experiment WITH punctuations]\n",
    "for i in range(len(persuasion_cleaned_sentences)):\n",
    "    persuasion_cleaned_sentences[i] = re.sub(\"[.;`()\\[\\]!,-?:&\\\"']\", \"\", persuasion_cleaned_sentences[i])\n",
    "#for i in range(len(persuasion_cleaned_sentences)):\n",
    "#    persuasion_cleaned_sentences[i] = re.sub(\"[']\", \"\", persuasion_cleaned_sentences[i])\n",
    "#    persuasion_cleaned_sentences[i] = re.sub(r\"([.;`()\\[\\]!,\\-?:&\\\"'])\", r\" \\1 \", persuasion_cleaned_sentences[i])\n",
    "\n",
    "# 7. get rid of new line characters\n",
    "for i in range(len(persuasion_cleaned_sentences)):\n",
    "    persuasion_cleaned_sentences[i] = persuasion_cleaned_sentences[i].replace(\"\\n\", \" \")\n",
    "# 8. get rid of empty sentences\n",
    "persuasion_cleaned_sentences = [sent for sent in persuasion_cleaned_sentences if sent.strip() != '']\n",
    "# 9. Surround your data sentences with `<s>` and `</s>`\n",
    "for i in range(len(persuasion_cleaned_sentences)):\n",
    "    persuasion_cleaned_sentences[i] = \"<s> \" + persuasion_cleaned_sentences[i] + \" </s>\"\n",
    "# 10. get rid of extra spaces (yeah... I know this wouldn't have matter much later but... I want a clean print)\n",
    "for i in range(len(persuasion_cleaned_sentences)):\n",
    "    persuasion_cleaned_sentences[i] = re.sub(\" +\", \" \", persuasion_cleaned_sentences[i])\n",
    "\n",
    "# print a few sents to make sure everything looks good\n",
    "for sent in persuasion_cleaned_sentences[:4]:\n",
    "    print(sent)\n",
    "    print()\n",
    "# LOOK AT HOW BEAUTIFUL THE NORMALIZED TEXT IS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T00:45:57.709632300Z",
     "start_time": "2023-11-23T00:45:57.659111300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the lengths of the top 12 longest sentences by words (with small snippets from the sentence):\n",
      "#1: 195 [<s> for though shy he did not seem reserved it had rather the appearan... ruly were the very feelings which ought to taste it but sparingly </s>]\n",
      "#2: 179 [<s> therefore sir walter what i would take leave to suggest is that if... ill bring me over at any time to save you the trouble of replying </s>]\n",
      "#3: 161 [<s> by the report which he hastened over to kellynch to make admiral c... mself every proof of his being a most responsible eligible tenant </s>]\n",
      "#4: 155 [<s> till he came and had examined the child their apprehensions were t... ng their male acquaintance who had been at all a favourite before </s>]\n",
      "#5: 149 [<s> the scenes in its neighbourhood charmouth with its high grounds an... be visited and visited again to make the worth of lyme understood </s>]\n",
      "#6: 145 [<s> there was a momentary expression in captain wentworths face at thi... ration for all that was real and unabsurd in the parents feelings </s>]\n",
      "#7: 138 [<s> she was entreated to give them as much of her time as possible inv... ance to the pump room could not but have her moments of imagining </s>]\n",
      "#8: 138 [<s> anne felt that she did not belong to the conversation and yet as c... uld not give could be properly interesting only to the principals </s>]\n",
      "#9: 136 [<s> uppercross was a moderatesized village which a few years back had ...  premises of the great house about a quarter of a mile farther on </s>]\n",
      "#10: 132 [<s> lady russell convinced that anne would not be allowed to be of any... try did not think that everything considered she wished to remain </s>]\n",
      "#11: 130 [<s> it was a very fine november day and the miss musgroves came throug... hing being to be done together however undesired and inconvenient </s>]\n",
      "#12: 130 [<s> she had previously in the anticipation of their marriage been very... t her at least the comfort of telling the whole story her own way </s>]\n"
     ]
    }
   ],
   "source": [
    "persuasion_cleaned_sentences = sorted(persuasion_cleaned_sentences, key=lambda x: len(x.split(' ')), reverse=True)\n",
    "print(\"Here are the lengths of the top 12 longest sentences by words (with small snippets from the sentence):\")\n",
    "for i in range(12):\n",
    "    sent_len = len(persuasion_cleaned_sentences[i].split(' '))\n",
    "    sent = persuasion_cleaned_sentences[i].replace(\"\\n\", \"\")\n",
    "    print(f\"#{i+1}: {sent_len} [{sent[:70]}... {sent[-70:]}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T00:45:57.709632300Z",
     "start_time": "2023-11-23T00:45:57.700631600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "persuasion_trimmed_sentences = [sent for sent in persuasion_cleaned_sentences]\n",
    "# I've decided to get rid of the top 6 sentences since they are significantly larger than those that follow\n",
    "for i in range(6):\n",
    "    persuasion_trimmed_sentences.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T00:45:57.709632300Z",
     "start_time": "2023-11-23T00:45:57.700631600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', 'she', 'was', 'entreated', 'to', 'give', 'them', 'as', 'much', 'of', 'her', 'time', 'as', 'possible', 'invited', 'for', 'every', 'day', 'and', 'all', 'day', 'long', 'or', 'rather', 'claimed', 'as', 'part', 'of', 'the', 'family', 'and', 'in', 'return', 'she', 'naturally', 'fell', 'into', 'all', 'her', 'wonted', 'ways', 'of', 'attention', 'and', 'assistance', 'and', 'on', 'charless', 'leaving', 'them', 'together', 'was', 'listening', 'to', 'mrs', 'musgroves', 'history', 'of', 'louisa', 'and', 'to', 'henriettas', 'of', 'herself', 'giving', 'opinions', 'on', 'business', 'and', 'recommendations', 'to', 'shops', 'with', 'intervals', 'of', 'every', 'help', 'which', 'mary', 'required', 'from', 'altering', 'her', 'ribbon', 'to', 'settling', 'her', 'accounts', 'from', 'finding', 'her', 'keys', 'and', 'assorting', 'her', 'trinkets', 'to', 'trying', 'to', 'convince', 'her', 'that', 'she', 'was', 'not', 'illused', 'by', 'anybody', 'which', 'mary', 'well', 'amused', 'as', 'she', 'generally', 'was', 'in', 'her', 'station', 'at', 'a', 'window', 'overlooking', 'the', 'entrance', 'to', 'the', 'pump', 'room', 'could', 'not', 'but', 'have', 'her', 'moments', 'of', 'imagining', '</s>'], ['<s>', 'anne', 'felt', 'that', 'she', 'did', 'not', 'belong', 'to', 'the', 'conversation', 'and', 'yet', 'as', 'captain', 'harville', 'seemed', 'thoughtful', 'and', 'not', 'disposed', 'to', 'talk', 'she', 'could', 'not', 'avoid', 'hearing', 'many', 'undesirable', 'particulars', 'such', 'as', 'how', 'mr', 'musgrove', 'and', 'my', 'brother', 'hayter', 'had', 'met', 'again', 'and', 'again', 'to', 'talk', 'it', 'over', 'what', 'my', 'brother', 'hayter', 'had', 'said', 'one', 'day', 'and', 'what', 'mr', 'musgrove', 'had', 'proposed', 'the', 'next', 'and', 'what', 'had', 'occurred', 'to', 'my', 'sister', 'hayter', 'and', 'what', 'the', 'young', 'people', 'had', 'wished', 'and', 'what', 'i', 'said', 'at', 'first', 'i', 'never', 'could', 'consent', 'to', 'but', 'was', 'afterwards', 'persuaded', 'to', 'think', 'might', 'do', 'very', 'well', 'and', 'a', 'great', 'deal', 'in', 'the', 'same', 'style', 'of', 'openhearted', 'communication', 'minutiae', 'which', 'even', 'with', 'every', 'advantage', 'of', 'taste', 'and', 'delicacy', 'which', 'good', 'mrs', 'musgrove', 'could', 'not', 'give', 'could', 'be', 'properly', 'interesting', 'only', 'to', 'the', 'principals', '</s>']]\n"
     ]
    }
   ],
   "source": [
    "persuasion_cleaned_sentences_and_words = [sent.split(' ') for sent in persuasion_trimmed_sentences]\n",
    "persuasion_cleaned_and_trimmed_sentences_and_words = [sent.split(' ') for sent in persuasion_trimmed_sentences]\n",
    "print(persuasion_cleaned_and_trimmed_sentences_and_words[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T00:45:57.794169100Z",
     "start_time": "2023-11-23T00:45:57.700631600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New largest sentence is 138 words long!\n"
     ]
    }
   ],
   "source": [
    "largest_word_len = len(persuasion_cleaned_and_trimmed_sentences_and_words[0])\n",
    "print(f'New largest sentence is {len(persuasion_trimmed_sentences[0].split(\" \"))} words long!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T00:45:59.707511900Z",
     "start_time": "2023-11-23T00:45:57.741633500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# load the glove models\n",
    "# this actually takes a while so ill save the result\n",
    "if does_var_exists(F'glove_model_200d'):\n",
    "    glove_model = load_var(F'glove_model_200d')\n",
    "else:\n",
    "    glove_dataset_dir = F'./data/glove/glove.6B.200d.txt'\n",
    "    glove_output_vec_dir = F'./data/glove/glove.6B.200d.wv'\n",
    "    if not os.path.isfile(glove_output_vec_dir):\n",
    "        glove2word2vec(glove_dataset_dir, glove_output_vec_dir)\n",
    "    glove_model = KeyedVectors.load_word2vec_format(glove_output_vec_dir, binary=False)\n",
    "    dump_var(F'glove_model_200d', glove_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T00:45:59.708514200Z",
     "start_time": "2023-11-23T00:45:59.706510400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The glove model has 400000 unique words.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The glove model has {len(glove_model.key_to_index)} unique words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T00:46:00.036049500Z",
     "start_time": "2023-11-23T00:45:59.707511900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# note I purposely run this on persuasion_cleaned_sentences_and_words instead of persuasion_cleaned_and_trimmed_sentences_and_words to extract\n",
    "# every ounce of juicy data I possibily can MUHAHAHAHAAA\n",
    "persuasion_model = Word2Vec(sentences=persuasion_cleaned_sentences_and_words, vector_size=200, window=8, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T00:46:03.488491400Z",
     "start_time": "2023-11-23T00:46:00.045048500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5879/5879 [00:03<00:00, 1735.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# Update pre-trained GloVe embeddings with information from the new model\n",
    "original_glove_weight = 0.65\n",
    "persuasion_model_weight = 0.35\n",
    "# these weights are hyperparameters ^^\n",
    "\n",
    "updated_persuasion_model = persuasion_model.wv\n",
    "assert original_glove_weight + persuasion_model_weight == 1\n",
    "for word in tqdm(updated_persuasion_model.key_to_index.keys()):\n",
    "    # we update over the persuasion model because the glove model has loads of words not in the novel\n",
    "    if word in glove_model:\n",
    "        updated_persuasion_model[word] = (glove_model[word] * original_glove_weight + updated_persuasion_model[word] * persuasion_model_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T00:46:03.488491400Z",
     "start_time": "2023-11-23T00:46:03.478488100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updated_persuasion_model['<pad>'] = np.zeros(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T01:10:28.024485200Z",
     "start_time": "2023-11-23T01:10:28.007482Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The persuasion model has 5880 unique words.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The persuasion model has {len(persuasion_model.wv.key_to_index)} unique words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T01:52:02.915711400Z",
     "start_time": "2023-11-23T01:52:02.892189500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentenceToEncodedMatrix(sentence: List[str]):  # sentences are lists of words\n",
    "    matrix = [updated_persuasion_model[w] for w in sentence]\n",
    "    # add padding to get all sequences to be the same length as the largest sequence\n",
    "    for k in range(largest_word_len - len(matrix) - 1):\n",
    "        matrix.append(updated_persuasion_model['<pad>'])\n",
    "    return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T01:52:03.529129Z",
     "start_time": "2023-11-23T01:52:03.510127600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3916029 , -0.08400654,  0.12170675, ..., -0.44900903,\n",
       "         0.2971122 , -0.22716783],\n",
       "       [ 0.30091184,  0.03857985, -0.12475669, ...,  0.09270309,\n",
       "         0.2641821 , -0.09325014],\n",
       "       [-0.04010855, -0.10371926, -0.20975363, ...,  0.16260943,\n",
       "        -0.5140166 , -0.0282568 ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenceToEncodedMatrix(['<s>', 'anne', 'felt', 'that', 'she', 'did', 'not', 'belong', 'to', 'the', 'conversation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T01:52:14.626872900Z",
     "start_time": "2023-11-23T01:52:12.840574400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3472/3472 [00:01<00:00, 2255.85it/s]\n"
     ]
    }
   ],
   "source": [
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in tqdm(range(len(persuasion_cleaned_and_trimmed_sentences_and_words))):\n",
    "    # remove the last word from the input\n",
    "    input_seq.append( sentenceToEncodedMatrix(persuasion_cleaned_and_trimmed_sentences_and_words[i][:-1]) )\n",
    "    # remove the first word from the target\n",
    "    target_seq.append( sentenceToEncodedMatrix(persuasion_cleaned_and_trimmed_sentences_and_words[i][1:]) )\n",
    "\n",
    "    assert len(input_seq[-1]) == largest_word_len - 1\n",
    "\n",
    "input_seq = np.array(input_seq)\n",
    "target_seq = np.array(target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T01:52:14.924104800Z",
     "start_time": "2023-11-23T01:52:14.890577400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5880"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(updated_persuasion_model.key_to_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T01:52:15.758129600Z",
     "start_time": "2023-11-23T01:52:15.356173100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_seq_tensor = torch.Tensor(input_seq).type(torch.DoubleTensor)\n",
    "target_seq_tensor = torch.Tensor(target_seq).type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T01:52:15.820707800Z",
     "start_time": "2023-11-23T01:52:15.754341600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([3472, 137, 200])\n",
      "target shape: torch.Size([3472, 137, 200])\n"
     ]
    }
   ],
   "source": [
    "print('input shape:', input_seq_tensor.shape)\n",
    "print('target shape:', target_seq_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T01:52:16.461992900Z",
     "start_time": "2023-11-23T01:52:16.445921700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3472"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PersuasionDataset(Dataset):\n",
    "\n",
    "        def __init__(self, input_words, target_words):\n",
    "            assert len(input_words) == len(target_words)\n",
    "            self.input_words = input_words\n",
    "            self.target_words = target_words\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.input_words)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.input_words[idx], self.target_words[idx]\n",
    "\n",
    "ds = PersuasionDataset(input_seq_tensor,target_seq_tensor)\n",
    "ds.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T01:52:17.060486700Z",
     "start_time": "2023-11-23T01:52:17.042485Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "data_loader = DataLoader(ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T01:52:17.993865400Z",
     "start_time": "2023-11-23T01:52:17.952858200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T01:52:55.490533400Z",
     "start_time": "2023-11-23T01:52:55.442533600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_layers,dropout):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        self.lstm = nn.LSTM(200, hidden_dim, n_layers,dropout=dropout,batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(hidden_dim, 200)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        hidden_state_size = x.size(0)\n",
    "\n",
    "        x = x.to(torch.double)\n",
    "\n",
    "        h0 = torch.zeros(self.n_layers,hidden_state_size,self.hidden_dim).double().to(device)\n",
    "        c0 = torch.zeros(self.n_layers,hidden_state_size,self.hidden_dim).double().to(device)\n",
    "\n",
    "        self.lstm = self.lstm.double()\n",
    "\n",
    "        self.fc1 = self.fc1.double()\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, (hx,cx) = self.lstm(x, (h0,c0))\n",
    "\n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T01:53:01.891637900Z",
     "start_time": "2023-11-23T01:53:01.869619600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (lstm): LSTM(200, 256, batch_first=True)\n",
      "  (fc1): Linear(in_features=256, out_features=200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = Model(hidden_dim=256, n_layers=1,dropout=0.0)\n",
    "print(model)\n",
    "model = model.double().to(device)\n",
    "# Define Loss, Optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001,weight_decay=0.0)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T01:53:17.318749700Z",
     "start_time": "2023-11-23T01:53:04.697134100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:12<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "model.train()\n",
    "\n",
    "# 5880 vocab size\n",
    "# 128 batch size\n",
    "# 138 seq length\n",
    "# 17664 = batch size * seq length\n",
    "# 200 vector dim\n",
    "# todo: map the output vector to the closest word\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for input_sequences, target_sequences in data_loader:\n",
    "        input_sequences = input_sequences.to(device)\n",
    "        target_sequences = target_sequences.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_sequences)\n",
    "\n",
    "        loss = loss_fn(output, target_sequences.view(-1, 200))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T01:53:23.717565600Z",
     "start_time": "2023-11-23T01:53:23.623049400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14fac3a9fe10>]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T01:53:26.045768900Z",
     "start_time": "2023-11-23T01:53:26.039767500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax_with_temperature(vec, temperature):\n",
    "    sum_exp = sum(math.exp(x/temperature) for x in vec)\n",
    "    return [math.exp(x/temperature)/sum_exp for x in vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T01:53:51.365373300Z",
     "start_time": "2023-11-23T01:53:51.351371400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('anybodys', 0.6871936917304993)]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_input = sentenceToEncodedMatrix(['<s>', 'anne', 'felt', 'that', 'she', 'did', 'not', 'belong', 'to'])\n",
    "test_input = torch.from_numpy(np.array([test_input])).to(device)\n",
    "result = model(test_input)[-1]\n",
    "print(updated_persuasion_model.similar_by_vector(result.detach().cpu().numpy(), topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T01:23:01.286346Z",
     "start_time": "2023-11-23T01:23:01.240829900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'char2int' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-1a9e369baf0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mint2char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Of \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-108-1a9e369baf0d>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, ch)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# One-hot encoding our input to fit into the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar2int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint2OneHot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-1a9e369baf0d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# One-hot encoding our input to fit into the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar2int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint2OneHot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'char2int' is not defined"
     ]
    }
   ],
   "source": [
    "temperature = 0.3\n",
    "\n",
    "def predict(model, ch):\n",
    "\n",
    "    # only look at last sample_len - 1 characters\n",
    "\n",
    "    ch = ch[-(largest_word_len - 1):]\n",
    "\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    ch = np.array([char2int(c) for c in ch])\n",
    "    ch = np.array([int2OneHot(ch, num_chars)])\n",
    "    ch = torch.from_numpy(ch).to(device)\n",
    "\n",
    "    out = model(ch)\n",
    "\n",
    "    # take the probability distribution of the last character in the sequence produced by the model\n",
    "    prob = softmax_with_temperature(out[-1],temperature)\n",
    "\n",
    "    # Choosing a character based on the probability distribution, with temperature\n",
    "    char_ind = choice(list(range(num_chars)), p=prob)\n",
    "\n",
    "    return int2char(char_ind)\n",
    "\n",
    "predict(model,\"Of \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnB5z-ZoZEa5"
   },
   "source": [
    "### Beam Search\n",
    "\n",
    "Beam search was described, and example shown, in Lecture 14. Here is a brief pseudo-code explaination of what\n",
    "you need to do:\n",
    "\n",
    "1. Develop your code as described above so that it can generate single sentences;\n",
    "2. Copy enough of your code over from HW 03 so that you can calculate the perplexity of\n",
    "        sentences (using the entire novel, or perhaps even a number of Jane Austen's novels as\n",
    "        the data source). As an alternative, you may wish to do this separately, store the nested dictionary\n",
    "        using Pickle, and load it here. \n",
    "3. Calculate the probability distribution of sentences in your data source that you used in the previous step, similar to what you did at the end of HW 01. \n",
    "4. Create a \"goodness function\" which estimates the quality of a sentence as the perplexity times the probability of its length.  This will be applied to all sequences of words, and not just sentences, but as a first approximation this is a way to attempt to make the distribution of sentence lengths similar to that in the novel.\n",
    "5. Follow the description in slide 7 of Lecture 14 to generate until you have 10 finished sentences. Print these out with their perplexity, probability of their length, and the combined goodness metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Describe what experiments you did with various alternatives as described above, and cut and paste examples illustrating your results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Three:  Part-of-Speech Tagging (40 pts)\n",
    "\n",
    "In this problem, we will experiment with three different approaches to the POS tagging problem, using\n",
    "the Brown Corpus as our data set. \n",
    "\n",
    "Before starting this problem, please review Lecture 13 and download the file <a href=\"Viterbi.ipynb\">Viterbi.ipynb</a> from the \n",
    "class web site. \n",
    "\n",
    "There are four parts to this problem:\n",
    "\n",
    "- Part A: You will establish a baseline accuracy for the task. \n",
    "- Part B: Using the implementation of the Viterbi algorithm for Hidden Markov Models you downloaded, you will determine how much better than the baseline you can do with this very standard method.\n",
    "- Part C: You will repeat the exercise of Part B, but using an LSTM implementation, exploring several options for the implementation of the LSTM layer.\n",
    "- Part D: You will evaluate your results, comparing the various methods in the context of the baseline method from Part A.\n",
    "- Optional: You may wish to try the same task with a transformer such as Bert. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the Brown Corpus has a list of all sentences tagged with parts of speech. The tags are\n",
    "a bit odd, and not generally used any more, so we will use a much simpler set of tags the `universal_tagset`. \n",
    "\n",
    "If you run the following cells, you will see that there are 57,340 sentences, tagged with 12 different tags. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# The first time you will need to download the corpus:\n",
    "\n",
    "from nltk.corpus import brown\n",
    " \n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "tagged_sentences = brown.tagged_sents(tagset='universal')\n",
    "\n",
    "print(f'There are {len(tagged_sentences)} sentences tagged with universal POS tags in the Brown Corpus.')\n",
    "print(\"\\nHere is the first sentence with universal tags:\",tagged_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Uncomment to see the complete list of tags. \n",
    "\n",
    "all_tagged_words = np.concatenate(tagged_sentences)\n",
    "all_tags = sorted(set([pos for (w,pos) in all_tagged_words]))\n",
    "print(f'There are {len(all_tags)} universal tags in the Brown Corpus.')\n",
    "print(all_tags)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnB5z-ZoZEa5"
   },
   "source": [
    "### Part A\n",
    "\n",
    "In this part, you will establish a baseline for the task, using the naive method suggested on slide 35 of Lecture 13:\n",
    "\n",
    "- Tag every word with its most frequent POS tag (for example, if 'recent' is most frequently tagged as 'ADJ', then assume that every time 'recent' appears in a sentence, it should be tagged with 'ADJ'); \n",
    "- If a word has two or more most frequent tags, choose the one that appears first in the list of sorted tags above. \n",
    "\n",
    "Note that there will not be any \"unknown words.\" \n",
    " \n",
    "Use this method to determine your baseline accuracy (it may not be 92% as reported on slide 35!):\n",
    "\n",
    "- Build a dictionary mapping every word to its most frequent tag;\n",
    "- Go through the entire tagged corpus, and report the accuracy (percentage of correct tags) of this baseline method. \n",
    "\n",
    "Do not tokenize or lower-case the words. Use the words and tags exactly as they are in the tagged sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B:  \n",
    "\n",
    "Now, review the `Viterbi.ipynb` notebook and read through Section 8.4 in Jurafsky & Martin to understand the basic approach that is used in the \"Janet will back the bill\" example. In detail:\n",
    "\n",
    "- Cut and paste the code from the Viterby notebook below and run your experiments in this notebook. \n",
    "- You need to calculate from the Brown Corpus tagged sentences the probabilities for the various matrices used as input to the method:\n",
    "   - `start_p`: This is the probability that a sentence starts with a given POS (in Figure 8.12 in J & M, this is given as the first line, in the row for `<s>`; simply collect the statistics for the first word in each sentence; it will be of size 1 x 12. \n",
    "   - `trans_p`: This is the matrix of probabilities that one POS follows another in a sentence; build a 12 x 12 matrix of frequencies for whether the column POS follows the row POS in a sentence and then normalize each row so that it is a probability distribution (each row should add to 1.0)\n",
    "   - `emit_p`: This is a matrix of size 12 x N, where N is the number of unique words in the corpus, which for each POS (the row) gives the probability that this POS in the output sequence corresponds to a specific word (the column) in the input sequence; again, you should collect frequency statistics about the relationship between POS and words, and normalize so that every row sums to 1.0. \n",
    "   \n",
    "Then run the algorithm on all the sentences in the tagged corpus, and determine the accuracy of the Viterbi algorithm. Again, the accuracy is calculated on each word, not on sentences as a whole. \n",
    "\n",
    "Report your results as a raw accuracy score, and in the two ways that were suggested on slide 12 of Lecture 11: percentage above the baseline established in Part A, and Cohen's Kappa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi code should be pasted here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C:  \n",
    "\n",
    "Next, you will need to develop an LSTM model to solve this problem. You may find it useful to\n",
    "refer to the following, which presents an approach in Keras.\n",
    "\n",
    "https://www.kaggle.com/code/tanyadayanand/pos-tagging-using-rnn/notebook\n",
    "\n",
    "\n",
    "You must do the following for this part:\n",
    "\n",
    "- Develop your code in Pytorch (of course!);\n",
    "- Use pretrained GloVe embeddings of dimension 200 and update them with the brown sentences; if you run into problems with RAM, you may use a smaller embedding dimension; \n",
    "- Truncate all sentences to a maximum of length 100 tokens, and pad shorter sentences (as in the reference above);\n",
    "- Use an LSTM model and try several different choices for the parameters to the layer:\n",
    "  - `hidden_size`:  Try several different widths for the layer\n",
    "  - `bidirectional`: Try unidirectional (False) and bidirectional (True)\n",
    "  - `num_layers`: Try 1 layer and 2 layers\n",
    "  - `dropout`: In the case of 2 layers, try several different dropouts, including 0.\n",
    "- Use early stopping with `patience = 50`;  \n",
    "You do not have to try every possible combination of these parameter choices; a good strategy is to\n",
    "try them separately, and then try a couple of combinations of the best choices of each. \n",
    "\n",
    "It is your choice about the other hyperparameters.  \n",
    "\n",
    "Provide a brief discussion of what you discovered, your best loss and accuracy measures for\n",
    "validation, and three versions of your testing accuracy, as in Part B.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide an analysis of what experiments you conducted with hyperparameters, what your results were, and in particular comment on how the two methods compare, especially given that one has *no* choice of hyperparameters, and one has *many* choices of parameters. How useful was the flexibility of choice in hyperparameters in Part C?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional:\n",
    "\n",
    "You might want to try doing this problem with a transformer model such as BERT. There are plenty of blog posts out there describing the details, and, as usual, chatGPT would have plenty of things to say about the topic.... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
