{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 505 Homework 05:  Recurrent Neural Networks\n",
    "\n",
    "#### Due Friday  11/17 at midnight (1 minute after 11:59 pm) in Gradescope (with a grace period of 6 hours)\n",
    "#### You may submit the homework up to 24 hours late (with the same grace period) for a penalty of 10%. \n",
    "\n",
    "All homeworks will be scored with a maximum of 100 points; point values are given\n",
    "for individual problems, and if parts of problems do not have point values given, they\n",
    "will be counted equally toward the total for that problem. \n",
    "\n",
    "Note: This homework is a bit different from the first four in this class in that in some parts we are specified **what** you need to do for your solutions, but much less of the **how** you write the details of the code. There are three reasons for this:\n",
    "\n",
    "- In a graduate level CS class, after four homeworks and two months of lectures, you should be well-equipped to work out the coding issues for yourself, and in general, going forward, this is how you will solve the kinds of problems presented here; \n",
    "- Suggestions for resources (mostly ML blogs) will be suggested; there are many resources, but these are from bloggers that I trust and have used in the past;\n",
    "- I am expecting that you will make good use of chatGPT for help with the details of syntax and low-level organization of your code. There is often nothing very stimulating or informative about precisely what is the syntax needed for a particular kind of layer in a network, and rather than poke around on StackOverflow, chatGPT is particularly good at summarizing existing approaches to ML coding tasks. \n",
    "\n",
    "#### Submission Instructions\n",
    "\n",
    "You must complete the homework by editing <b>this notebook</b> and submitting the following two files in Gradescope by the due date and time:\n",
    "\n",
    "  - A file <code>HW05.ipynb</code> (be sure to select <code>Kernel -> Restart and Run All</code> before you submit, to make sure everything works); and\n",
    "  - A file <code>HW05.pdf</code> created from the previous.\n",
    "  \n",
    "  For best results obtaining a clean PDF file on the Mac, select <code>File -> Print Review</code> from the Jupyter window, then choose <code>File-> Print</code> in your browser and then <code>Save as PDF</code>.  Something  similar should be possible on a Windows machine -- just make sure it is readable and no cell contents have been cut off. Make it easy to grade!\n",
    "  \n",
    "The date and time of your submission is the last file you submitted, so if your IPYNB file is submitted on time, but your PDF is late, then your submission is late. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborators (5 pts)\n",
    "\n",
    "Describe briefly but precisely\n",
    "\n",
    "1. Any persons you discussed this homework with and the nature of the discussion;\n",
    "2. Any online resources you consulted and what information you got from those resources; and\n",
    "3. Any AI agents (such as chatGPT or CoPilot) or other applications you used to complete the homework, and the nature of the help you received. \n",
    "\n",
    "A few brief sentences is all that I am looking for here. \n",
    "\n",
    "    <Your answer here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T21:24:44.595366200Z",
     "start_time": "2023-11-24T21:24:44.555834600Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch._C'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset,DataLoader\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random_split,Dataset,DataLoader\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_osp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mthroughput_benchmark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThroughputBenchmark\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpp_backtrace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_cpp_backtrace\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_registration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rename_privateuse1_backend, generate_methods_for_privateuse1_backend\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/throughput_benchmark.py:2\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_time\u001b[39m(time_us\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, time_ms\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, time_s\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Defines how to format time'''\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch._C'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import shuffle, seed, choice\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split,Dataset,DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, brown\n",
    "import pickle\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing import Dict\n",
    "from typing import Any\n",
    "from typing import Union\n",
    "from typing import DefaultDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T21:24:27.180960500Z",
     "start_time": "2023-11-24T21:24:27.172443900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell contains some helpful methods to dump notebook variables\n",
    "to a file so you don't have to rerun expensive computations every\n",
    "time.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "def does_var_exists(var_name: str) -> bool:\n",
    "    return os.path.isfile(F'./data/pickle/{var_name}.pkl')\n",
    "\n",
    "def dump_var(var_name: str, obj) -> None:\n",
    "    with open(F'./data/pickle/{var_name}.pkl', 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "\n",
    "def load_var(var_name: str) -> Union[None, object]:\n",
    "    if not does_var_exists(var_name):\n",
    "        return None\n",
    "    with open(F'./data/pickle/{var_name}.pkl', 'rb') as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-24T21:14:12.940567300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dirs where I'll be storing my data\n",
    "os.makedirs('./data/glove', exist_ok=True)\n",
    "os.makedirs('./data/java', exist_ok=True)\n",
    "os.makedirs('./data/pickle', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem One:  Character-Level Generative Model (20 pts)\n",
    "\n",
    "A basic character-level model has been provided on the class web site in the row for Lecture 14: \n",
    "<a href=\"https://www.cs.bu.edu/fac/snyder/cs505/CharacterLevelLSTM.ipynb\">IPYNB</a>. Your first step is to download this and run it in Colab (or download the data file, which is in the CS 505 Data Directory and also linked on the web site, and run it on your local machine) and understand all its various features. Most of it is straight-forward at this point in the course, but the definition of the model is a bit messy, and you will need to read about LSTM layers in the Pytorch documents to really understand what it is doing and what the hyperparameters mean. \n",
    "\n",
    "Also take a look at the article \"The Unreasonable Effectiveness of Recurrent Neural Networks\" linked with lecture 14. \n",
    "\n",
    "For this problem, you will run this code on a dataset consisting of Java code files, which has been uploaded to the CS 505 Data Directory and also to the class web site: <a href=\"https://www.cs.bu.edu/fac/snyder/cs505/JavaFiles/\">DIR</a>  Select some number of these files and concatenate them into one long text file, such that you have approximately 10-20K characters (if you have trouble running out of RAM you can use fewer, but try to get at least 10K). \n",
    "\n",
    "You will run the character-level model on this dataset. You may either cut and paste code into this notebook, or submit the file with your changes and output along with this notebook to Gradescope.\n",
    "\n",
    "Your task is to get a character-level model that has not simply memorized the Java text file by overfitting, and does not do much other than spit out random characters (underfitting).  You will get the former if you simply run it for many epochs without any changes to the hyperparameters; you will get the latter if you run it only a few epochs. \n",
    "\n",
    "You should experiment with different hyperparameters, which in the notebook are indicated\n",
    "by \n",
    "\n",
    "          <== something to play with\n",
    "\n",
    "and try to get a model that seems to recognize typical Java syntax such as comments, matching parentheses, expressions, assignments, and formatting, but is not just repeating\n",
    "exact text from the data file. Clearly, the number of epochs plays a crucial role, but I also want you to\n",
    "experiment with the various hyperparameters to try to avoid overfitting. See my lectures on T 10/31 and Th 11/2 (recorded and on my YT channel) for the background to this.\n",
    "\n",
    "Note that the code you will work from does not use validation and testing sets, nor does it calculate the accuracy, but only tracks the loss. The nature of the data sets for character-level models does not seem to lend itself to accuracy metrics, but you may wish to try this -- I have not found it to be useful, but have simply focussed on the output and \"eyeballed\" the results to determine how much they have generalized\n",
    "from the data. \n",
    "\n",
    "Submit your notebook(s) to Gradescope as usual, and also provide a summary of your results in the next cell. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:13.967754600Z",
     "start_time": "2023-11-22T03:12:13.916415100Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the java sources\n",
    "java_source_list = []\n",
    "num_chars = 0\n",
    "\n",
    "files = os.listdir('./data/java/')\n",
    "random.shuffle(files)\n",
    "for java_file in files:\n",
    "    # only add full files (because I don't want training code that doesn't have closing brackets\n",
    "    with open('./data/java/' + java_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        num_chars += len('\\n'.join(lines))\n",
    "        # don't go over 32k characters (note: I deviate a bit from the \"10k to 20k\" suggestion)\n",
    "        if num_chars > 32000:\n",
    "            break\n",
    "        java_source_list += lines\n",
    "\n",
    "java_source_text = '\\n'.join(java_source_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:14.722643100Z",
     "start_time": "2023-11-22T03:12:14.704643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is 31509 characters long.\n",
      "First 200 characters: /* File: RecursiveGraphics.java\n",
      "\n",
      " * Author: \n",
      "\n",
      " * Date: \n",
      "\n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      "\n",
      " */\n",
      "\n",
      "\n",
      "\n",
      "import java.awt.Color;\n",
      "\n",
      "import java.awt.Canvas;\n",
      "\n",
      "import java .awt.Graphics;\n",
      "\n",
      "import\n"
     ]
    }
   ],
   "source": [
    "print(f\"Text is {len(java_source_text)} characters long.\")\n",
    "print(f\"First 200 characters: {java_source_text[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:18.062743Z",
     "start_time": "2023-11-22T03:12:18.045604500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# do some cleaning\n",
    "cleaned_java_source_text = java_source_text\n",
    "# remove double new lines\n",
    "cleaned_java_source_text = re.sub(\"\\n\\n\", \"\\n\", cleaned_java_source_text)\n",
    "# remove lines with only whitespace\n",
    "cleaned_java_source_text = re.sub(r'^\\s*$', \"\", cleaned_java_source_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:18.636454900Z",
     "start_time": "2023-11-22T03:12:18.623476100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 200 characters (cleaned): /* File: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "import java .awt.Graphics;\n",
      "import javax.sw\n"
     ]
    }
   ],
   "source": [
    "print(f\"First 200 characters (cleaned): {cleaned_java_source_text[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:19.167158Z",
     "start_time": "2023-11-22T03:12:19.038641400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 89 unique characters in the text.\n",
      "Character set: ['\\t', '\\n', ' ', '!', '\"', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}'].\n"
     ]
    }
   ],
   "source": [
    "chars_in_text = sorted(list(set(cleaned_java_source_text)))\n",
    "num_chars = len(chars_in_text)\n",
    "\n",
    "print(f'There are {num_chars} unique characters in the text.')\n",
    "print(f'Character set: {chars_in_text}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:19.621073200Z",
     "start_time": "2023-11-22T03:12:19.609877300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create functions mapping characters to integers and back\n",
    "\n",
    "def char2int(c):\n",
    "    return chars_in_text.index(c)\n",
    "\n",
    "def int2char(i):\n",
    "    return chars_in_text[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:27.863164900Z",
     "start_time": "2023-11-22T03:12:27.847164700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HYPER PARAMETERS HERE\n",
    "sample_len = 160 # <== something to play with\n",
    "batch_size = 512 # <== something to play with\n",
    "model_dropout= 0.4 # <== something to play with\n",
    "hidden_dim_size = 316 # <== something to play with\n",
    "n_layers_count = 3 # <== something to play with\n",
    "leaning_rate = 0.001 # <== something to play with\n",
    "weight_decay = 0.00015 # <== something to play with\n",
    "model_temp = 0.7 # <== something to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:28.551460500Z",
     "start_time": "2023-11-22T03:12:28.521724300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence:\n",
      "/* File: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "i\n",
      "Target sequence:\n",
      "* File: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "im\n",
      "\n",
      "Input sequence:\n",
      "* File: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "im\n",
      "Target sequence:\n",
      " File: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "imp\n",
      "\n",
      "Input sequence:\n",
      " File: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "imp\n",
      "Target sequence:\n",
      "File: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "impo\n",
      "\n",
      "Input sequence:\n",
      "File: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "impo\n",
      "Target sequence:\n",
      "ile: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "impor\n",
      "\n",
      "Input sequence:\n",
      "ile: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "impor\n",
      "Target sequence:\n",
      "le: RecursiveGraphics.java\n",
      " * Author: \n",
      " * Date: \n",
      " * Purpose: This is the template for PS5, Problem 6\n",
      " */\n",
      "\n",
      "import java.awt.Color;\n",
      "import java.awt.Canvas;\n",
      "import\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating lists that will hold our input and target sample sequences\n",
    "\n",
    "input_seq_chars = []\n",
    "target_seq_chars = []\n",
    "\n",
    "for k in range(len(cleaned_java_source_text)-sample_len+1):\n",
    "\n",
    "    # Remove last character for input sequence\n",
    "    input_seq_chars.append(cleaned_java_source_text[k:k+sample_len-1])\n",
    "\n",
    "    # Remove firsts character for target sequence\n",
    "    target_seq_chars.append(cleaned_java_source_text[k+1:k+sample_len])\n",
    "\n",
    "for i in range(5):\n",
    "    print(f'Input sequence:\\n{input_seq_chars[i]}')\n",
    "    print(f'Target sequence:\\n{target_seq_chars[i]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:29.412658900Z",
     "start_time": "2023-11-22T03:12:29.398628100Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert an integer into a one-hot encoding of the given size (= number of characters)\n",
    "def int2OneHot(X,size):\n",
    "\n",
    "    def int2OneHot1(x,size=10):\n",
    "        tmp = np.zeros(size)\n",
    "        tmp[int(x)] = 1.0\n",
    "        return tmp\n",
    "\n",
    "    return np.array([ int2OneHot1(x, size) for x in X ]).astype('double')\n",
    "\n",
    "# do the same thing, but for a list/array of integers\n",
    "def seq2OneHot(seq,size):\n",
    "    return np.array([ int2OneHot(x, size) for x in seq ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:42.269289600Z",
     "start_time": "2023-11-22T03:12:30.094474100Z"
    }
   },
   "outputs": [],
   "source": [
    "input_seq = []\n",
    "for i in range(len(input_seq_chars)):\n",
    "    input_seq.append( [char2int(ch) for ch in input_seq_chars[i]])\n",
    "input_seq = seq2OneHot(input_seq,size=num_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:54.400574800Z",
     "start_time": "2023-11-22T03:12:42.290470100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_seq = []\n",
    "for i in range(len(input_seq_chars)):\n",
    "    target_seq.append([char2int(ch) for ch in target_seq_chars[i]])\n",
    "target_seq = seq2OneHot(target_seq,size=num_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:54.409578600Z",
     "start_time": "2023-11-22T03:12:54.399573300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: (30480, 159, 89)\n",
      "target shape: (30480, 159, 89)\n"
     ]
    }
   ],
   "source": [
    "print('input shape:', input_seq.shape)\n",
    "print('target shape:', target_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:57.392332Z",
     "start_time": "2023-11-22T03:12:54.407597200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_seq = torch.Tensor(input_seq).type(torch.DoubleTensor)\n",
    "target_seq = torch.Tensor(target_seq).type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:57.405332200Z",
     "start_time": "2023-11-22T03:12:57.396331200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30480"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Basic_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # return a pair x,y at the index idx in the data set\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "ds = Basic_Dataset(input_seq,target_seq)\n",
    "ds.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:57.454515600Z",
     "start_time": "2023-11-22T03:12:57.419568500Z"
    }
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:57.465516600Z",
     "start_time": "2023-11-22T03:12:57.427154700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:57.480663900Z",
     "start_time": "2023-11-22T03:12:57.454515600Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, dropout):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers,dropout=dropout,batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        hidden_state_size = x.size(0)\n",
    "\n",
    "        x = x.to(torch.double)\n",
    "\n",
    "        h0 = torch.zeros(self.n_layers,hidden_state_size,self.hidden_dim).double().to(device)\n",
    "        c0 = torch.zeros(self.n_layers,hidden_state_size,self.hidden_dim).double().to(device)\n",
    "\n",
    "        self.lstm = self.lstm.double()\n",
    "\n",
    "        self.fc1 = self.fc1.double()\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, (hx,cx) = self.lstm(x, (h0,c0))\n",
    "\n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:57.483659200Z",
     "start_time": "2023-11-22T03:12:57.480663900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (lstm): LSTM(89, 316, num_layers=3, batch_first=True, dropout=0.4)\n",
      "  (fc1): Linear(in_features=316, out_features=89, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "\n",
    "model = Model(input_size=num_chars, output_size=num_chars, hidden_dim=hidden_dim_size, n_layers=n_layers_count,dropout=model_dropout)\n",
    "\n",
    "print(model)\n",
    "\n",
    "model = model.double().to(device)\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=leaning_rate,weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:12:57.532379700Z",
     "start_time": "2023-11-22T03:12:57.480663900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:24:33.755824Z",
     "start_time": "2023-11-22T03:22:58.652442300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:34<00:00, 23.74s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "    for input_seq_batch,target_seq_batch in data_loader:\n",
    "        input_seq_batch = input_seq_batch.to(device)\n",
    "        target_seq_batch = target_seq_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        target_seq_hat = model(input_seq_batch)\n",
    "        loss = loss_fn(target_seq_hat,target_seq_batch.view(-1,num_chars))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:24:38.801343Z",
     "start_time": "2023-11-22T03:24:37.856672200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14d55d511a58>]"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:48:00.682335800Z",
     "start_time": "2023-11-22T03:48:00.538733800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#file_path = './data/pickle/final_model.torch'\n",
    "#torch.save(model, file_path)\n",
    "#dump_var(\"losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:24:43.140354800Z",
     "start_time": "2023-11-22T03:24:42.997791600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of softmax with temperature.\n",
      "distribution: [0.1, 0.3, 0.6]\n",
      "[1.9287498479637375e-22, 9.3576229688393e-14, 0.9999999999999064]\n",
      "[0.006377460922442302, 0.04712341652466416, 0.9464991225528936]\n",
      "[0.06289001324586753, 0.1709527801977903, 0.7661572065563421]\n",
      "[0.12132647558421489, 0.23631170657656433, 0.6423618178392208]\n",
      "[0.2583896517379799, 0.3155978333128144, 0.4260125149492058]\n",
      "[0.3255767455856355, 0.3321538321280155, 0.3422694222863489]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def softmax_with_temperature(vec, temperature):\n",
    "    sum_exp = sum(math.exp(x/temperature) for x in vec)\n",
    "    return [math.exp(x/temperature)/sum_exp for x in vec]\n",
    "\n",
    "print(\"Example of softmax with temperature.\")\n",
    "dist = [0.1, 0.3, 0.6]\n",
    "print('distribution:',dist)\n",
    "print(softmax_with_temperature(dist,0.01))\n",
    "print(softmax_with_temperature(dist,0.1))\n",
    "print(softmax_with_temperature(dist,0.2))\n",
    "print(softmax_with_temperature(dist,0.3))\n",
    "print(softmax_with_temperature(dist,1))\n",
    "print(softmax_with_temperature(dist,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:24:44.182602700Z",
     "start_time": "2023-11-22T03:24:43.915040800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature = model_temp\n",
    "\n",
    "def predict(model, ch):\n",
    "\n",
    "    # only look at last sample_len - 1 characters\n",
    "\n",
    "    ch = ch[-(sample_len - 1):]\n",
    "\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    ch = np.array([char2int(c) for c in ch])\n",
    "    ch = np.array([int2OneHot(ch, num_chars)])\n",
    "    ch = torch.from_numpy(ch).to(device)\n",
    "\n",
    "    out = model(ch)\n",
    "\n",
    "    # take the probability distribution of the last character in the sequence produced by the model\n",
    "    prob = softmax_with_temperature(out[-1],temperature)\n",
    "\n",
    "    # Choosing a character based on the probability distribution, with temperature\n",
    "    char_ind = choice(list(range(num_chars)), p=prob)\n",
    "\n",
    "    return int2char(char_ind)\n",
    "\n",
    "predict(model,\"public static \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:24:46.819524500Z",
     "start_time": "2023-11-22T03:24:46.817521400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample(model, out_len, start):\n",
    "    model.eval() # eval mode\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size):\n",
    "        char = predict(model, chars)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T03:25:18.549852300Z",
     "start_time": "2023-11-22T03:24:46.819524500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public static void drawSpiralHelper(double x, double y, double s, boolean horizontal, int depth, Graphics g) {\n",
      " \t\n",
      "\tif(depth == 0) {\n",
      "           return;\n",
      "       }\n",
      "       \n",
      "       // draw lines\n",
      "       \n",
      "       double x1 = x0 + len * Math.cos(angle);\n",
      "       double y1 = y0 - len * Math.sin(angle);\n",
      " \n",
      "       drawLine(x0,y0,x1,y1,g);\n",
      "       \n",
      "       for (int i = 0; i < rDelta.length; ++i) {\n",
      "          changeColorByRecursionDepth(depth); \n",
      "          drawTreeHelper(x1, y1, len * lenDelta[i], angle + thetaDelta[i], depth-1,g);\n",
      "       }\n",
      "   }\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    //  Problem 6.1    Your Turn!\n",
      "    //  For this one, you have to figure aut how to do the recursive cales on tem .\n",
      "    public void drawLine(double x1, double y1, double x5, double y5, int depth, Graphics g) {\n",
      "       if (depth == 2) {\n",
      "\t    drawLine(x1, y1, x5, y5, g);\n",
      "       }\n",
      "       else {\n",
      "           \n",
      "           double x2, x3, x4;\n",
      "           double y2, y1, y3, y5;  \n",
      "            \n",
      "           double deltaX = x5 - x1;\n",
      "           double deltaY = y5 - x1;\n",
      "           double deltaY = y5 - y1;\n",
      "           \n",
      "           x2 = x1 + deltaX/3.0;\n",
      "           y2 = y1 + deltaY/3.0;\n",
      "           x4 = y5 - deltaY/3.0;\n",
      "           x3 = ((x1 + x5)/2.0 - (Math.sqrt(3.0)/6.0) * (y5 - y5));\n",
      "           double y3 = ((y1 + y5)/2.0 + (Math.sqrt(3.0)/6.0) * (y5 - y5));\n",
      "           y3 = ((y1 + y5)/2 + ( (Math.sqrt(3.0)/6.0) * (y5 - y5));\n",
      "\t   \n",
      "           double x3 = ((x1 + x5)/2.0 - (Math.sqrt(3.0)/6.0) * (y1 - y1));\n",
      "           x3 = ((x1 + x5)/2.0 + (Math.sqrt(3.0)/6.0) * (x1 - y1));\n",
      "           x3 = (y1 + deltaX/3.0;\n",
      "           x2 =  ( x1 + x) --  Math.sqrt(3.0);\n",
      "        double y[] = y    Math.sqrt(3; \n",
      "        double[] ys = new double[3];\n",
      "        xs[0] = x; \n",
      "        ys[0] = y;\n",
      "        ys[1] = x-s/2;\n",
      "        ys[2] = y+s/2;\n",
      "        ys[2] = y+s*0.66;\n",
      "        x1 = y + Meth.sqrt(atth).0.5 * Math.cIs40;\n",
      "        if(depth == 0) {\n",
      "           return;\n",
      "       }\n",
      "       \n",
      "       // draw lines\n",
      "       \n",
      "       double x1 = x0 - len * Math.cos(angle);\n",
      "       double y1 = y0 - len * Math.sin(angle);\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, 2000, \"public static \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnB5z-ZoZEa5"
   },
   "source": [
    "### Your analysis\n",
    "\n",
    "Please describe your experiments and cut and paste various outputs to show how the model performed at\n",
    "various numbers of epochs and with various hyperparameters. What characteristics of Java was it able to learn? What did it not learn? The article \"The Unreasonable ...\" does a nice job of showing this kind of behavior as the number of epochs increases, and you might look at it before writing your answer here. \n",
    "\n",
    "My goal going into this was to create a model that understands Java syntax (including at the very least, opening and closing brackets) as stated in the problem.\n",
    "\n",
    "Starting with the example code in the CharacterLevelLSTM.ipynb, my first attempts to get an effective model were disappointing. With my initial hyperparameters copied from the CharacterLevelLSTM.ipynb, my model was memorizing things way too quick (in less than 20 epochs). Here were some results from 50 epochs:\n",
    "Code Generations: https://share.aseef.dev/bDfMCkP\n",
    "Losses: https://share.aseef.dev/WHcLkjc\n",
    "\n",
    "In order to prevent memorization I played around with several things:\n",
    "a) Used drop-out rates from 30%-50% to prevent memorization\n",
    "b) Played around with layers low as 1 and deep as 6 in hopes the model can better extract abstract info\n",
    "c) Increased batch size (though this was just to train faster as my model size grew)\n",
    "d) Reduced the number of feature states by half in hopes that the model won't have enough states to memorize the data (from 96-256) - but this didn't work well in practice.\n",
    "e) Also played around with the sample lengths and found though higher sample lengths took longer to train, they produced higher quality models\n",
    "\n",
    "In this experimentation, the challenges I encountered were that the dropout rates were simply too high on one spectrum and the model just stopped learning after one point (no matter how many more epochs I would train it for):\n",
    "Code Generation: https://share.aseef.dev/coGIakH\n",
    "Losses: https://share.aseef.dev/Er2mKLm\n",
    "\n",
    "But simply just reducing dropout rates didn't help much either (with a dropout rate of 0.3):\n",
    "Code Generation: https://share.aseef.dev/LMdsJCE\n",
    "Losses: https://share.aseef.dev/rHDRPH7\n",
    "\n",
    "In the end, nothing worked and I was not able to produce any \"decent\" models. My models were either garbage, or memorized. That was until I took a page from \"the unreasonable effectiveness of data\" and simply added about 50% more data by increasing my data size to up to 32k characters!\n",
    "\n",
    "My final model finally produces outputs that aren't fully memorized and finally starts to demonstrate some understanding of brackets, indenting (mostly), methods, function calls and etc. In fact, the model went beyond my expectation and the code it wrote was still relevant to the name of the method (e.i. wasn't making just completely random calls).\n",
    "Code Generation: https://share.aseef.dev/i9WE5Tg\n",
    "Losses: https://share.aseef.dev/NoAtm44\n",
    "\n",
    "I think the final drop out rates I ended up using (at 40%) was also an important choice that helped the model finally generalize to a satisfactory model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Two:  Word-Level Generative Model (40 pts)\n",
    "\n",
    "In this problem you will write another generative model, as you did in HW 03, but this time you will use an LSTM network, GloVe word embeddings, and beam search. \n",
    "\n",
    "Before you start, read the following blog post to see the core ideas involved in creating a generative model using word embeddings:\n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/\n",
    "\n",
    "You may also wish to consult with chatGPT about how to develop this kind of model in Pytorch.\n",
    "\n",
    "The requirements for this problem are as follows (they mostly consist of the extensions proposed at\n",
    "the end of the blog post linked above):\n",
    "\n",
    "- Develop your code in Pytorch, not Keras\n",
    "- Use the novel *Persuation* by Jane Austen as your training data (available through the Brown Corpus); if you have trouble with RAM you will need to cut down the number of sentences (perhaps by eliminating the longest sentences as well, see next point). \n",
    "- Develop a sentence-level model by padding sentences to the maximum sentence length in the novel (if this seems extreme, you may wish to delete a small number of the longest sentences to reduce the maximum length). Surround your data sentences with `<s>` and `</s>` and your model should generate one sentence at a time (as you did in HW 03), i.e., it should stop if it generates the `</s>` token. \n",
    "- Use pretrained GLoVe embeddings with dimension 200, and update them (refine by training further) on the sentences in the novel; if you have trouble with RAM you may use a smaller dimension. \n",
    "- Experiment with the hyperparameters (sample length, number of layers, uni- or bi-directional, weight_decay, dropout, number of epochs, temperature of the softmax, etc.) as you did in Problem One to find the \"sweet spot\" where you are generating interesting-looking sentences but not simply repeating sentences from the data. You may want to try adding more linear layers on top to pick the most likely next word. \n",
    "- Generate sentences using Beam Search, which we describe below. \n",
    "\n",
    "Your solution should be the code, samples of sentences generated with their score (described below), and your description of the investigation of various hyperparameters, and what strategy ended up seeming to generate the most realistic sentences that were not simply a repeat of sentences in the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T21:14:24.636415100Z",
     "start_time": "2023-11-24T21:14:24.598412700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#if does_var_exists('persuasion_doc'):\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#    persuasion_doc = load_var('persuasion_doc')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# download it from gutenberg\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgutenberg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the text of Persuasion by Jane Austen\u001b[39;00m\n\u001b[1;32m      7\u001b[0m persuasion_raw_text \u001b[38;5;241m=\u001b[39m gutenberg\u001b[38;5;241m.\u001b[39mraw(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mausten-persuasion.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "#if does_var_exists('persuasion_doc'):\n",
    "#    persuasion_doc = load_var('persuasion_doc')\n",
    "#else:\n",
    "# download it from gutenberg\n",
    "nltk.download('gutenberg')\n",
    "# Load the text of Persuasion by Jane Austen\n",
    "persuasion_raw_text = gutenberg.raw('austen-persuasion.txt')\n",
    "# Tokenize the text into sentences\n",
    "#spacy.require_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "persuasion_doc = nlp(persuasion_raw_text)\n",
    "dump_var('persuasion_doc', persuasion_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T21:14:25.000532400Z",
     "start_time": "2023-11-24T21:14:24.973529900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'persuasion_raw_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe novel is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mpersuasion_raw_text\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m characters long with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(persuasion_raw_text\u001b[38;5;241m.\u001b[39mlower()))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m uncased unique characters \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mset\u001b[39m(persuasion_raw_text\u001b[38;5;241m.\u001b[39mlower())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'persuasion_raw_text' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"The novel is {len(persuasion_raw_text)} characters long with {len(set(persuasion_raw_text.lower()))} uncased unique characters {set(persuasion_raw_text.lower())}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T21:14:25.177111900Z",
     "start_time": "2023-11-24T21:14:25.168109800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# convert doc to a string list of sentences\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m persuasion_sentences: \u001b[43mList\u001b[49m[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m persuasion_doc\u001b[38;5;241m.\u001b[39msents:\n\u001b[1;32m      4\u001b[0m     persuasion_sentences \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [sent\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__str__\u001b[39m()]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "# convert doc to a string list of sentences\n",
    "persuasion_sentences: List[List[str]] = []\n",
    "for sent in persuasion_doc.sents:\n",
    "    persuasion_sentences += [sent.__str__()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:37:56.953392500Z",
     "start_time": "2023-11-24T11:37:56.938395700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Persuasion by Jane Austen 1818]\n",
      "\n",
      "\n",
      "Chapter 1\n",
      "\n",
      "\n",
      "Sir Walter Elliot, of Kellynch Hall, in Somersetshire, was a man who,\n",
      "for his own amusement, never took up any book but the Baronetage;\n",
      "there he found occupation for an idle hour, and consolation in a\n",
      "distressed one; there his faculties were roused into admiration and\n",
      "respect, by contemplating the limited remnant of the earliest patents;\n",
      "there any unwelcome sensations, arising from domestic affairs\n",
      "changed naturally into pity and contempt as he turned over\n",
      "the almost endless creations of the last century; and there,\n",
      "if every other leaf were powerless, he could read his own history\n",
      "with an interest which never failed.  \n",
      "This was the page at which\n",
      "the favourite volume always opened:\n",
      "\n",
      "           \"ELLIOT OF KELLYNCH HALL.\n",
      "\n",
      "\n",
      "\"Walter Elliot, born March 1, 1760, married, July 15, 1784, Elizabeth,\n",
      "daughter of James Stevenson, Esq. of South Park, in the county of\n",
      "Gloucester, by which lady (who died 1800) he has issue Elizabeth,\n",
      "born June 1, 1785; Anne, born August 9, 1787; a still-born son,\n",
      "November 5, 1789; Mary, born November 20, 1791.\n"
     ]
    }
   ],
   "source": [
    "# print first 3 sentences to confirm stuff works\n",
    "for sent in persuasion_sentences[:3]:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:37:57.012867200Z",
     "start_time": "2023-11-24T11:37:56.964397Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> this was the page at which the favourite volume always opened elliot of kellynch hall </s>\n",
      "\n",
      "<s> walter elliot born march 1 1760 married july 15 1784 elizabeth daughter of james stevenson esq of south park in the county of gloucester by which lady who died 1800 he has issue elizabeth born june 1 1785 anne born august 9 1787 a stillborn son november 5 1789 mary born november 20 1791 </s>\n",
      "\n",
      "<s> precisely such had the paragraph originally stood from the printers hands but sir walter had improved it by adding for the information of himself and his family these words after the date of marys birth married december 16 1810 charles son and heir of charles musgrove esq of uppercross in the county of somerset and by inserting most accurately the day of the month on which he had lost his wife </s>\n",
      "\n",
      "<s> then followed the history and rise of the ancient and respectable family in the usual terms how it had been first settled in cheshire how mentioned in dugdale serving the office of high sheriff representing a borough in three successive parliaments exertions of loyalty and dignity of baronet in the first year of charles ii with all the marys and elizabeths they had married forming altogether two handsome duodecimo pages and concluding with the arms and mottoprincipal seat kellynch hall in the county of somerset and sir walters handwriting again in this finale heir presumptive william walter elliot esq great grandson of the second sir walter </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets clean up the sentences...\n",
    "persuasion_cleaned_sentences = persuasion_sentences\n",
    "# 1. get rid of the book title\n",
    "persuasion_cleaned_sentences: List[str] = persuasion_cleaned_sentences[1:]\n",
    "# 2. get rid of chapter titles\n",
    "for i in range(len(persuasion_cleaned_sentences)):\n",
    "    persuasion_cleaned_sentences[i] = re.sub(\"Chapter [0-9]+\", \"\", persuasion_cleaned_sentences[i])\n",
    "# 3. get rid of trailing whitespaces\n",
    "for i in range(len(persuasion_cleaned_sentences)):\n",
    "    persuasion_cleaned_sentences[i] = persuasion_cleaned_sentences[i].strip()\n",
    "# 4. lowercase everything\n",
    "for i in range(len(persuasion_cleaned_sentences)):\n",
    "    persuasion_cleaned_sentences[i] = persuasion_cleaned_sentences[i].lower()\n",
    "# 5. convert contractions\n",
    "for i in range(len(persuasion_cleaned_sentences)):\n",
    "    persuasion_cleaned_sentences[i] = persuasion_cleaned_sentences[i].replace(\"'ll\", \" will\")\n",
    "    persuasion_cleaned_sentences[i] = persuasion_cleaned_sentences[i].replace(\"that's\", \"that is\")\n",
    "    persuasion_cleaned_sentences[i] = persuasion_cleaned_sentences[i].replace(\"n't\", \" not\")\n",
    "\n",
    "# 6 handle punctuations\n",
    "# no punctuations\n",
    "for i in range(len(persuasion_cleaned_sentences)):\n",
    "    persuasion_cleaned_sentences[i] = re.sub(\"[.;`()\\[\\]!,\\-?:&\\\"']\", \"\", persuasion_cleaned_sentences[i])\n",
    "# with punctuations\n",
    "#for i in range(len(persuasion_cleaned_sentences)):  # quotes at start of sents belong to the prev sent\n",
    "#    if persuasion_cleaned_sentences[i].strip().startswith(\"\\\"\"):\n",
    "#        persuasion_cleaned_sentences[i] = persuasion_cleaned_sentences[i].strip()[1:]\n",
    "#        persuasion_cleaned_sentences[i-1] = persuasion_cleaned_sentences[i-1] + \"\\\"\"\n",
    "#for i in range(len(persuasion_cleaned_sentences)):\n",
    "#    persuasion_cleaned_sentences[i] = re.sub(\"([.;`()\\[\\]!,\\-?:&\\\"'])\", r\" \\1 \", persuasion_cleaned_sentences[i])\n",
    "\n",
    "# 7. get rid of new line characters\n",
    "for i in range(len(persuasion_cleaned_sentences)):\n",
    "    persuasion_cleaned_sentences[i] = persuasion_cleaned_sentences[i].replace(\"\\n\", \" \")\n",
    "# 8. get rid of empty sentences\n",
    "persuasion_cleaned_sentences = [sent for sent in persuasion_cleaned_sentences if sent.strip() != '']\n",
    "# 9. Surround your data sentences with `<s>` and `</s>`\n",
    "for i in range(len(persuasion_cleaned_sentences)):\n",
    "    persuasion_cleaned_sentences[i] = \"<s> \" + persuasion_cleaned_sentences[i] + \" </s>\"\n",
    "# 10. get rid of extra spaces (yeah... I know this wouldn't have matter much later but... I want a clean print)\n",
    "for i in range(len(persuasion_cleaned_sentences)):\n",
    "    persuasion_cleaned_sentences[i] = re.sub(\" +\", \" \", persuasion_cleaned_sentences[i])\n",
    "\n",
    "# print a few sents to make sure everything looks good\n",
    "for sent in persuasion_cleaned_sentences[:4]:\n",
    "    print(sent)\n",
    "    print()\n",
    "# LOOK AT HOW BEAUTIFUL THE NORMALIZED TEXT IS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:37:57.012867200Z",
     "start_time": "2023-11-24T11:37:57.005869100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', 'this', 'was', 'the', 'page', 'at', 'which', 'the', 'favourite', 'volume', 'always', 'opened', 'elliot', 'of', 'kellynch', 'hall', '</s>'], ['<s>', 'walter', 'elliot', 'born', 'march', '1', '1760', 'married', 'july', '15', '1784', 'elizabeth', 'daughter', 'of', 'james', 'stevenson', 'esq', 'of', 'south', 'park', 'in', 'the', 'county', 'of', 'gloucester', 'by', 'which', 'lady', 'who', 'died', '1800', 'he', 'has', 'issue', 'elizabeth', 'born', 'june', '1', '1785', 'anne', 'born', 'august', '9', '1787', 'a', 'stillborn', 'son', 'november', '5', '1789', 'mary', 'born', 'november', '20', '1791', '</s>']]\n"
     ]
    }
   ],
   "source": [
    "persuasion_cleaned_sentences_and_words = [sent.split(' ') for sent in persuasion_cleaned_sentences]\n",
    "#persuasion_cleaned_and_trimmed_sentences_and_words = [sent.split(' ') for sent in persuasion_trimmed_sentences]\n",
    "print(persuasion_cleaned_sentences_and_words[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:37:57.079870400Z",
     "start_time": "2023-11-24T11:37:57.018865200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The novel has 5958 unique words!\n"
     ]
    }
   ],
   "source": [
    "words_set = set(' '.join(persuasion_cleaned_sentences).split(' '))\n",
    "print(f\"The novel has {len(words_set)} unique words!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T20:36:56.686803900Z",
     "start_time": "2023-11-24T20:36:56.671423900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HYPER PARAMETERS\n",
    "sequence_length = 9\n",
    "batch_size = 256\n",
    "hidden_dimensions = 384\n",
    "num_layers = 3\n",
    "dropout = 0.12\n",
    "learning_rate = 0.001\n",
    "adam_weight_decay = 0.0\n",
    "bidirectional = False\n",
    "\n",
    "update_embeddings = False  # NOTE: technically, the instructions say to update the embeddings. I tried it. I got poor results.\n",
    "                           # In such case, prof. Snyder has said he doesn't mind if we deviate from the instructions a bit.\n",
    "glove_embedding_dim = 300  # NOTE: I am using the 300D vectors instead of the 200D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:37:58.736072500Z",
     "start_time": "2023-11-24T11:37:57.059871Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# load the glove models\n",
    "# this actually takes a while so ill save the result\n",
    "if does_var_exists(F'glove_model_{glove_embedding_dim}d'):\n",
    "    glove_model = load_var(F'glove_model_{glove_embedding_dim}d')\n",
    "else:\n",
    "    glove_dataset_dir = F'./data/glove/glove.6B.{glove_embedding_dim}d.txt'\n",
    "    glove_output_vec_dir = F'./data/glove/glove.6B.{glove_embedding_dim}d.wv'\n",
    "    if not os.path.isfile(glove_output_vec_dir):\n",
    "        glove2word2vec(glove_dataset_dir, glove_output_vec_dir)\n",
    "    glove_model = KeyedVectors.load_word2vec_format(glove_output_vec_dir, binary=False)\n",
    "    dump_var(F'glove_model_{glove_embedding_dim}d', glove_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:37:58.737069700Z",
     "start_time": "2023-11-24T11:37:58.736072500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The glove model has 400000 unique words.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The glove model has {len(glove_model.key_to_index)} unique words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:37:59.032016700Z",
     "start_time": "2023-11-24T11:37:58.736072500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# just a hack that I use in the next cell to trim the glove model to only include words and terms that also exist in the novel\n",
    "updated_persuasion_model = Word2Vec(sentences=persuasion_cleaned_sentences_and_words, vector_size=glove_embedding_dim, window=10, min_count=1, workers=4).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:05.223438900Z",
     "start_time": "2023-11-24T11:37:59.034038800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5958/5958 [00:06<00:00, 969.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# Update pre-trained GloVe embeddings removing words that dont exist in the novel\n",
    "for word in tqdm(updated_persuasion_model.key_to_index.keys()):\n",
    "    # we update over the persuasion model because the glove model has loads of words not in the novel\n",
    "    if word in glove_model:\n",
    "        updated_persuasion_model[word] = glove_model[word]\n",
    "    else:\n",
    "        # in theory, these weights shouldnt matter much because they will be updated anyways\n",
    "        updated_persuasion_model[word] = updated_persuasion_model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:05.225437400Z",
     "start_time": "2023-11-24T11:38:05.223438900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5958/5958 [00:00<00:00, 1972816.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# And for words in the novel but not in the glove model, give them random weights\n",
    "for word in tqdm(words_set):\n",
    "    if word not in updated_persuasion_model.key_to_index:\n",
    "        updated_persuasion_model[word] = np.random.rand(glove_embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:05.225437400Z",
     "start_time": "2023-11-24T11:38:05.223438900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = len(updated_persuasion_model.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:05.225437400Z",
     "start_time": "2023-11-24T11:38:05.223438900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated glove model has 5958 unique words.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The updated glove model has {vocab_size} unique words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:05.255466400Z",
     "start_time": "2023-11-24T11:38:05.223438900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# piazza post @464 implied we can do it another way and not using padding\n",
    "# I find using padding a waste of precious memory so... bye bye padding :)!\n",
    "# updated_persuasion_model['<pad>'] = np.zeros(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:05.281438300Z",
     "start_time": "2023-11-24T11:38:05.228435400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_to_index(word: str) -> int:\n",
    "    return updated_persuasion_model.key_to_index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:05.281438300Z",
     "start_time": "2023-11-24T11:38:05.269441700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def index_to_word(index: int) -> str:\n",
    "    return updated_persuasion_model.index_to_key[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:05.313947300Z",
     "start_time": "2023-11-24T11:38:05.269441700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now that we have updated glove, and we have a combined clean text, in order to extract the sequences\n",
    "# we need a big long text again\n",
    "\n",
    "tokens = ' '.join(persuasion_cleaned_sentences).split(' ')\n",
    "sequences = list()\n",
    "for i in range(sequence_length, len(tokens)):\n",
    "     # select sequence of tokens\n",
    "     seq = tokens[i-sequence_length:i]\n",
    "     # convert into a line\n",
    "     line = ' '.join(seq)\n",
    "     # store\n",
    "     sequences.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:05.313947300Z",
     "start_time": "2023-11-24T11:38:05.306946700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seqToWordIndex(sequence: List[str]):  # sequence are lists of words\n",
    "    matrix = [word_to_index(w) for w in sequence]\n",
    "    return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:05.327947600Z",
     "start_time": "2023-11-24T11:38:05.314946400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,   30,  147,   17,   11,   75,   16, 1678,    3,    2,  338])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqToWordIndex(['<s>', 'anne', 'felt', 'that', 'she', 'did', 'not', 'belong', 'to', 'the', 'conversation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:05.329947200Z",
     "start_time": "2023-11-24T11:38:05.323948900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seqsToWordIndex(sequences: List[List[str]]):\n",
    "    res = [seqToWordIndex(x) for x in sequences]\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:05.380983200Z",
     "start_time": "2023-11-24T11:38:05.333946500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seqToWordEmbeddingMatrix(sequence: List[str]):\n",
    "    embeddings = [updated_persuasion_model[word] for word in sequence]\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:05.380983200Z",
     "start_time": "2023-11-24T11:38:05.373976800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seqsToWordEmbeddingMatrix(sequences: List[List[str]]):\n",
    "    embeddings = [seqToWordEmbeddingMatrix(sequence) for sequence in sequences]\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:05.380983200Z",
     "start_time": "2023-11-24T11:38:05.373976800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def indexSeqToSent(index_seq: List[int]):\n",
    "    word_list = []\n",
    "    for i in index_seq:\n",
    "        word_list += [index_to_word(i)]\n",
    "    return ' '.join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:06.011225800Z",
     "start_time": "2023-11-24T11:38:05.415355800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert sequences of words into sequences of vectors that each represent a word\n",
    "split_sequences = [seq.split(' ') for seq in sequences]\n",
    "full_seq = seqsToWordIndex(split_sequences)\n",
    "input_seq = full_seq[:-1]\n",
    "target_seq = full_seq[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:06.014226Z",
     "start_time": "2023-11-24T11:38:06.012227800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_seq_tensor = torch.Tensor(input_seq).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:06.014226Z",
     "start_time": "2023-11-24T11:38:06.012227800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert vocab_size == len(updated_persuasion_model.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:40.805592100Z",
     "start_time": "2023-11-24T11:38:06.012227800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90108/90108 [00:17<00:00, 5154.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# This was a one-hot encoding for the vocab - takes a POOP LOAD of memory\n",
    "target_seq_tensor = []\n",
    "for i in tqdm(range(len(target_seq))):\n",
    "    target_seq_tensor += [F.one_hot(torch.Tensor(target_seq[i]).type(torch.LongTensor), num_classes=vocab_size)]\n",
    "target_seq_tensor = torch.stack(target_seq_tensor, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:40.808600100Z",
     "start_time": "2023-11-24T11:38:40.805592100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([90108, 9])\n",
      "target shape: torch.Size([90108, 9, 5958])\n"
     ]
    }
   ],
   "source": [
    "print('input shape:', input_seq_tensor.shape)\n",
    "print('target shape:', target_seq_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:40.813602200Z",
     "start_time": "2023-11-24T11:38:40.805592100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90108"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PersuasionDataset(Dataset):\n",
    "\n",
    "        def __init__(self, input_words, target_words):\n",
    "            assert len(input_words) == len(target_words)\n",
    "            self.input_words = input_words\n",
    "            self.target_words = target_words\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.input_words)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            # given the input, I'm trying to predict the last element of target_words\n",
    "            return self.input_words[idx], self.target_words[idx][-1]\n",
    "\n",
    "ds = PersuasionDataset(input_seq_tensor,target_seq_tensor)\n",
    "ds.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:40.822791300Z",
     "start_time": "2023-11-24T11:38:40.815600200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  51,    8,    2, 3136,   25,   35,    2, 1053, 1828])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:40.905349Z",
     "start_time": "2023-11-24T11:38:40.825781200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  51,    8,    2, 3136,   25,   35,    2, 1053, 1828])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:40.905349Z",
     "start_time": "2023-11-24T11:38:40.866779900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  51,    8,    2, 3136,   25,   35,    2, 1053, 1828])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq_tensor[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:40.905349Z",
     "start_time": "2023-11-24T11:38:40.866779900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 1,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seq_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:40.905349Z",
     "start_time": "2023-11-24T11:38:40.866779900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 1,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(input_seq_tensor[0], num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:40.910379100Z",
     "start_time": "2023-11-24T11:38:40.907352200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 1,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seq_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:40.910379100Z",
     "start_time": "2023-11-24T11:38:40.907352200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T14:35:47.402513500Z",
     "start_time": "2023-11-24T14:35:47.383512Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T20:37:06.029612500Z",
     "start_time": "2023-11-24T20:37:06.010610100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_layers, dropout, bidirectional):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(updated_persuasion_model.vectors), freeze=not update_embeddings)\n",
    "\n",
    "        #Defining the layers\n",
    "        self.lstm = nn.LSTM(glove_embedding_dim, hidden_dim, num_layers=n_layers, batch_first=True,\n",
    "                            dropout=dropout, bidirectional=bidirectional)\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, glove_embedding_dim)\n",
    "        self.fc2 = nn.Linear(glove_embedding_dim, vocab_size)\n",
    "\n",
    "\n",
    "    # written by chat gpt\n",
    "    # TODO: i dont understand this\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.n_layers * 2 if self.lstm.bidirectional else self.n_layers, x.size(0), self.hidden_dim).double().requires_grad_().to(device)\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.n_layers * 2 if self.lstm.bidirectional else self.n_layers, x.size(0), self.hidden_dim).double().requires_grad_().to(device)\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        ## Index hidden state of last time step\n",
    "        out = F.relu(self.fc1(out[:, -1, :]))  # Apply ReLU activation to fc1 output\n",
    "        out = self.fc2(out)  # No activation function applied to fc2\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T20:37:07.383647100Z",
     "start_time": "2023-11-24T20:37:07.132414400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (embedding): Embedding(5958, 300)\n",
      "  (lstm): LSTM(300, 384, num_layers=3, batch_first=True, dropout=0.12)\n",
      "  (fc1): Linear(in_features=384, out_features=300, bias=True)\n",
      "  (fc2): Linear(in_features=300, out_features=5958, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = Model(hidden_dim=hidden_dimensions, n_layers=num_layers,dropout=dropout, bidirectional=bidirectional)\n",
    "print(model)\n",
    "model = nn.DataParallel(model)\n",
    "model = model.double().to(device)\n",
    "# Define Loss, Optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()  # this loss function is better for continuous values and CrossEntropy is better for classification. Here we are trying to predict word vectors.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay=adam_weight_decay)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T21:12:08.679410700Z",
     "start_time": "2023-11-24T21:12:06.666010600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-e1dfef61cc79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/venv/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/venv/lib64/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/venv/lib64/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/venv/lib64/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0m_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_tup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1077\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1078\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "model.train()\n",
    "\n",
    "# 5880 vocab size\n",
    "# 128 batch size\n",
    "# 50 seq length\n",
    "# 17664 = batch size * seq length\n",
    "# 200 vector dim\n",
    "# todo: map the output vector to the closest word\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for input_sequences, target_sequences in data_loader:\n",
    "        input_sequences = input_sequences.to(device)\n",
    "        target_sequences = target_sequences.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_sequences)\n",
    "\n",
    "        loss = loss_fn(output, target_sequences.double())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T21:10:31.436610400Z",
     "start_time": "2023-11-24T21:10:31.345759Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Losses For Last 5 Epochs:  0.20598162772029727\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(losses)\n",
    "print(\"Avg Losses For Last 5 Epochs: \", statistics.mean(losses[-5:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "hyperparams: https://share.aseef.dev/SIPGFD0\n",
    "losses: https://share.aseef.dev/z9Tttmh\n",
    "sample: https://share.aseef.dev/fjVeQBg\n",
    "\n",
    "hyper params: https://share.aseef.dev/p5EFTKX\n",
    "losses: https://share.aseef.dev/h2W0Jjb\n",
    "sample: https://share.aseef.dev/e4GODeE\n",
    "\n",
    "hyper params: https://share.aseef.dev/g3Hayy1\n",
    "losses: https://share.aseef.dev/0qb4Whr\n",
    "sample: https://share.aseef.dev/2hXb1xL\n",
    "\n",
    "losses: https://share.aseef.dev/tyj2nW2\n",
    "sample: https://share.aseef.dev/jQdjwSM\n",
    "\n",
    "hyper params: https://share.aseef.dev/iydr0X1\n",
    "losses: https://share.aseef.dev/3l9G7Sy\n",
    "sample: https://share.aseef.dev/r6EFLAF\n",
    "\n",
    "hyper params: https://share.aseef.dev/AslX6Hh\n",
    "losses: https://share.aseef.dev/X3Nu3ab\n",
    "sample: https://share.aseef.dev/OBOP3VP\n",
    "\n",
    "model 6\n",
    "hyper params: https://share.aseef.dev/C9BCop0\n",
    "losses: https://share.aseef.dev/gSLHjYA\n",
    "samples: https://share.aseef.dev/2XDkHUW\n",
    "\n",
    "model 7\n",
    "hyper params: https://share.aseef.dev/uGCuo4V\n",
    "losses: https://share.aseef.dev/E9M5SJk\n",
    "samples 1: https://share.aseef.dev/T278p0o\n",
    "samples 2: https://share.aseef.dev/CAhfrWJ\n",
    "\n",
    "model 8\n",
    "bi dir\n",
    "hyper params: https://share.aseef.dev/lGWqFlT\n",
    "losses: https://share.aseef.dev/N6XWs9u\n",
    "samples 1: https://share.aseef.dev/4zGKK1Q\n",
    "\n",
    "model 9\n",
    "hyper params: https://share.aseef.dev/xd7v9Zv\n",
    "losses: https://share.aseef.dev/kSqJNJc\n",
    "samples: https://share.aseef.dev/kWxclDp\n",
    "trained for 750 epochs. Poor results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T21:10:45.989157500Z",
     "start_time": "2023-11-24T21:10:45.851080Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'problem_2_working.9.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T21:10:48.366164400Z",
     "start_time": "2023-11-24T21:10:48.347409700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax_with_temperature(vec, temperature):\n",
    "    sum_exp = sum(math.exp(x/temperature) for x in vec)\n",
    "    return [math.exp(x/temperature)/sum_exp for x in vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T21:10:48.783646700Z",
     "start_time": "2023-11-24T21:10:48.516985400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outer'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature = 0.7\n",
    "\n",
    "def split(raw_sent):\n",
    "    return raw_sent.lower().split(' ')\n",
    "\n",
    "def predict(model, word_seq):\n",
    "\n",
    "    # only look at last sequence_length - 1 characters\n",
    "    word_seq = word_seq[-(sequence_length - 1):]\n",
    "\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    word_seq = seqsToWordIndex([word_seq])\n",
    "    word_seq = torch.from_numpy(word_seq).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    out = model(word_seq)\n",
    "\n",
    "    # take the probability distribution of the last character in the sequence produced by the model\n",
    "    prob = softmax_with_temperature(out[-1],temperature)\n",
    "\n",
    "    # Choosing a character based on the probability distribution, with temperature\n",
    "    word_ind = choice(list(range(vocab_size)), p=prob)\n",
    "\n",
    "    return index_to_word(word_ind)\n",
    "\n",
    "predict(model, ['<s>', 'anne', 'felt', 'that', 'she', 'did', 'not', 'belong', 'to', 'the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T21:10:48.791647700Z",
     "start_time": "2023-11-24T21:10:48.786649500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample(model, out_len, start):\n",
    "    model.eval() # eval mode\n",
    "    # First off, run through the starting characters\n",
    "    words = [w for w in start]\n",
    "    size = out_len - len(words)\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in tqdm(range(size)):\n",
    "        a_word = predict(model, words)\n",
    "        words.append(a_word)\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T21:11:07.490503700Z",
     "start_time": "2023-11-24T21:10:49.241443600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:18<00:00,  3.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> anne felt that she did not belong to the outer house as they could be properly aware in the smallest interval of england however she did not to give her to enjoy the evening of frightened sensibility seldom added had found him </s> <s> they did not come out but he was looking that he should do this over the past </s> <s> i have no reason that his father are very well pleased with all other strangers and'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(model, 80, ['<s>', 'anne', 'felt', 'that', 'she', 'did', 'not', 'belong', 'to', 'the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnB5z-ZoZEa5"
   },
   "source": [
    "### Beam Search\n",
    "\n",
    "Beam search was described, and example shown, in Lecture 14. Here is a brief pseudo-code explaination of what\n",
    "you need to do:\n",
    "\n",
    "1. Develop your code as described above so that it can generate single sentences;\n",
    "2. Copy enough of your code over from HW 03 so that you can calculate the perplexity of\n",
    "        sentences (using the entire novel, or perhaps even a number of Jane Austen's novels as\n",
    "        the data source). As an alternative, you may wish to do this separately, store the nested dictionary\n",
    "        using Pickle, and load it here. \n",
    "3. Calculate the probability distribution of sentences in your data source that you used in the previous step, similar to what you did at the end of HW 01. \n",
    "4. Create a \"goodness function\" which estimates the quality of a sentence as the perplexity times the probability of its length.  This will be applied to all sequences of words, and not just sentences, but as a first approximation this is a way to attempt to make the distribution of sentence lengths similar to that in the novel.\n",
    "5. Follow the description in slide 7 of Lecture 14 to generate until you have 10 finished sentences. Print these out with their perplexity, probability of their length, and the combined goodness metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "# my code from HW3 for calculating perplexity was very long\n",
    "# luckily chat gpt wrote us a cleaner and shorter calculation\n",
    "def calculate_perplexity(sentence, model, n, alpha=0.4):\n",
    "    # Tokenize the sentence (adjust as per your preprocessing steps)\n",
    "    tokens = sentence.split()\n",
    "\n",
    "    # Create n-grams\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "\n",
    "    # Calculate perplexity with Stupid Backoff\n",
    "    perplexity = 1\n",
    "    for n_gram in n_grams:\n",
    "        prob = model.get_ngram_probability(n_gram)\n",
    "\n",
    "        if prob == 0:\n",
    "            # Stupid Backoff: Apply lower-order n-gram probability\n",
    "            lower_order_n_gram = n_gram[1:]\n",
    "            lower_order_prob = model.get_ngram_probability(lower_order_n_gram)\n",
    "            prob = alpha * lower_order_prob\n",
    "\n",
    "        perplexity *= 1 / prob\n",
    "\n",
    "    perplexity = math.pow(perplexity, 1 / -len(n_grams))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "N: the N in the N-gram (AKA how big of an N-gram to make)\n",
    "s: the sentence (or list of words) to process\n",
    "\n",
    "returns a list of n-tuples corresponding to the extracted n-grams\n",
    "\"\"\"\n",
    "def get_Ngrams_for_sentence(N: int, s: List[str]) -> List[tuple]:\n",
    "    N_gram : List[tuple] = []\n",
    "    for j in range(len(s) - N + 1): # the minus N + 1 to ensure all tuples are of size N\n",
    "        entry = []\n",
    "        for k in range(N):\n",
    "            entry.append(s[j + k])\n",
    "        N_gram += [tuple(entry)]\n",
    "    return N_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ngrams : List[List[tuple]] = list(ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_dict : List[Dict[tuple, int]] = [defaultdict(int) for _ in range(5)]\n",
    "for i in range(len(Ngrams)):\n",
    "    for val in Ngrams[i]:\n",
    "        freq_dict[i][val] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_Ngram_distribution(N: int):\n",
    "    distribution : Dict[str, float] = defaultdict(float)\n",
    "    for val in freq_dict[N]:\n",
    "        distribution[val] = freq_dict[N][val] / len(Ngrams[N])\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ngram_distribution = [[]]*5\n",
    "\n",
    "for N in range(1,5):\n",
    "    Ngram_distribution[N] = get_Ngram_distribution(N)\n",
    "\n",
    "Ngram_distribution[1][('<s>',)] = 1.0         # because every sentence will start with <s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PN(N: int,W: tuple):\n",
    "    assert N == len(W)\n",
    "    numerator = Ngram_distribution[N][W]\n",
    "    # no denominator for N=1\n",
    "    if N == 1:\n",
    "        return numerator\n",
    "    denominator = Ngram_distribution[N-1][W[:-1]]\n",
    "    if denominator == 0:\n",
    "        return 0\n",
    "    prob = numerator / denominator\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def P(N:int, W: tuple) -> float:\n",
    "    list_len = len(W)\n",
    "\n",
    "    win = 0\n",
    "    def increment_window():\n",
    "        return win + 1 if win + 1 <= N else N\n",
    "    if N != 1: # 1 is a special case\n",
    "        win = increment_window()\n",
    "\n",
    "    total_prob = 1\n",
    "    # build up to the nth gram\n",
    "    while win != N:\n",
    "        win = increment_window()\n",
    "        total_prob *= PN(win, W[:win])\n",
    "\n",
    "    for i in range(1, list_len - (N - 1)):\n",
    "        # for remaining terms, slide over the n-gram window until end\n",
    "        total_prob *= PN(win, W[i:i+win])\n",
    "    return total_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PN_with_stupid_backoff(N:int, W:tuple):\n",
    "    # must have N == len(W)\n",
    "    # special case: in case N > len(W), N will be reduced\n",
    "    if N > len(W):\n",
    "        N = len(W)\n",
    "    # the other way is not allowed\n",
    "    assert N == len(W), \"N=\" + str(N) + \", W=\" + str(W)\n",
    "\n",
    "    prob = PN(N, W)\n",
    "    # base case 1\n",
    "    if prob == 0 and N == 1:\n",
    "        return brown.words().count(W[0])/ len(brown.words()) # Piazza post @140\n",
    "    # base case 2\n",
    "    elif prob != 0:\n",
    "        return prob\n",
    "    # base case 3\n",
    "    else:\n",
    "        return PN_with_stupid_backoff(N - 1, W[1:]) * 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def P_stupid_backoff(N,W):\n",
    "    list_len = len(W)\n",
    "\n",
    "    win = 0\n",
    "    def increment_window():\n",
    "        return win + 1 if win + 1 <= N else N\n",
    "    if N != 1: # 1 is a special case\n",
    "        win = increment_window()\n",
    "\n",
    "    total_prob = 1\n",
    "    # build up to the nth gram\n",
    "    while win != N:\n",
    "        win = increment_window()\n",
    "        total_prob *= PN_with_stupid_backoff(win, W[:win])\n",
    "\n",
    "    for i in range(1, list_len - (N - 1)):\n",
    "        # for remaining terms, slide over the n-gram window until end\n",
    "        total_prob *= PN_with_stupid_backoff(win, W[i:i+win])\n",
    "    return total_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PP(N,W):\n",
    "    return P_stupid_backoff(N, W) ** -(1/len(W[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Describe what experiments you did with various alternatives as described above, and cut and paste examples illustrating your results.\n",
    "\n",
    "With bad word embeddings, model struggled to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Three:  Part-of-Speech Tagging (40 pts)\n",
    "\n",
    "In this problem, we will experiment with three different approaches to the POS tagging problem, using\n",
    "the Brown Corpus as our data set. \n",
    "\n",
    "Before starting this problem, please review Lecture 13 and download the file <a href=\"Viterbi.ipynb\">Viterbi.ipynb</a> from the \n",
    "class web site. \n",
    "\n",
    "There are four parts to this problem:\n",
    "\n",
    "- Part A: You will establish a baseline accuracy for the task. \n",
    "- Part B: Using the implementation of the Viterbi algorithm for Hidden Markov Models you downloaded, you will determine how much better than the baseline you can do with this very standard method.\n",
    "- Part C: You will repeat the exercise of Part B, but using an LSTM implementation, exploring several options for the implementation of the LSTM layer.\n",
    "- Part D: You will evaluate your results, comparing the various methods in the context of the baseline method from Part A.\n",
    "- Optional: You may wish to try the same task with a transformer such as Bert. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the Brown Corpus has a list of all sentences tagged with parts of speech. The tags are\n",
    "a bit odd, and not generally used any more, so we will use a much simpler set of tags the `universal_tagset`. \n",
    "\n",
    "If you run the following cells, you will see that there are 57,340 sentences, tagged with 12 different tags. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# The first time you will need to download the corpus:\n",
    "\n",
    "from nltk.corpus import brown\n",
    " \n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "tagged_sentences = brown.tagged_sents(tagset='universal')\n",
    "\n",
    "print(f'There are {len(tagged_sentences)} sentences tagged with universal POS tags in the Brown Corpus.')\n",
    "print(\"\\nHere is the first sentence with universal tags:\",tagged_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Uncomment to see the complete list of tags. \n",
    "\n",
    "all_tagged_words = np.concatenate(tagged_sentences)\n",
    "all_tags = sorted(set([pos for (w,pos) in all_tagged_words]))\n",
    "print(f'There are {len(all_tags)} universal tags in the Brown Corpus.')\n",
    "print(all_tags)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnB5z-ZoZEa5"
   },
   "source": [
    "### Part A\n",
    "\n",
    "In this part, you will establish a baseline for the task, using the naive method suggested on slide 35 of Lecture 13:\n",
    "\n",
    "- Tag every word with its most frequent POS tag (for example, if 'recent' is most frequently tagged as 'ADJ', then assume that every time 'recent' appears in a sentence, it should be tagged with 'ADJ'); \n",
    "- If a word has two or more most frequent tags, choose the one that appears first in the list of sorted tags above. \n",
    "\n",
    "Note that there will not be any \"unknown words.\" \n",
    " \n",
    "Use this method to determine your baseline accuracy (it may not be 92% as reported on slide 35!):\n",
    "\n",
    "- Build a dictionary mapping every word to its most frequent tag;\n",
    "- Go through the entire tagged corpus, and report the accuracy (percentage of correct tags) of this baseline method. \n",
    "\n",
    "Do not tokenize or lower-case the words. Use the words and tags exactly as they are in the tagged sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B:  \n",
    "\n",
    "Now, review the `Viterbi.ipynb` notebook and read through Section 8.4 in Jurafsky & Martin to understand the basic approach that is used in the \"Janet will back the bill\" example. In detail:\n",
    "\n",
    "- Cut and paste the code from the Viterby notebook below and run your experiments in this notebook. \n",
    "- You need to calculate from the Brown Corpus tagged sentences the probabilities for the various matrices used as input to the method:\n",
    "   - `start_p`: This is the probability that a sentence starts with a given POS (in Figure 8.12 in J & M, this is given as the first line, in the row for `<s>`; simply collect the statistics for the first word in each sentence; it will be of size 1 x 12. \n",
    "   - `trans_p`: This is the matrix of probabilities that one POS follows another in a sentence; build a 12 x 12 matrix of frequencies for whether the column POS follows the row POS in a sentence and then normalize each row so that it is a probability distribution (each row should add to 1.0)\n",
    "   - `emit_p`: This is a matrix of size 12 x N, where N is the number of unique words in the corpus, which for each POS (the row) gives the probability that this POS in the output sequence corresponds to a specific word (the column) in the input sequence; again, you should collect frequency statistics about the relationship between POS and words, and normalize so that every row sums to 1.0. \n",
    "   \n",
    "Then run the algorithm on all the sentences in the tagged corpus, and determine the accuracy of the Viterbi algorithm. Again, the accuracy is calculated on each word, not on sentences as a whole. \n",
    "\n",
    "Report your results as a raw accuracy score, and in the two ways that were suggested on slide 12 of Lecture 11: percentage above the baseline established in Part A, and Cohen's Kappa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi code should be pasted here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C:  \n",
    "\n",
    "Next, you will need to develop an LSTM model to solve this problem. You may find it useful to\n",
    "refer to the following, which presents an approach in Keras.\n",
    "\n",
    "https://www.kaggle.com/code/tanyadayanand/pos-tagging-using-rnn/notebook\n",
    "\n",
    "\n",
    "You must do the following for this part:\n",
    "\n",
    "- Develop your code in Pytorch (of course!);\n",
    "- Use pretrained GloVe embeddings of dimension 200 and update them with the brown sentences; if you run into problems with RAM, you may use a smaller embedding dimension; \n",
    "- Truncate all sentences to a maximum of length 100 tokens, and pad shorter sentences (as in the reference above);\n",
    "- Use an LSTM model and try several different choices for the parameters to the layer:\n",
    "  - `hidden_size`:  Try several different widths for the layer\n",
    "  - `bidirectional`: Try unidirectional (False) and bidirectional (True)\n",
    "  - `num_layers`: Try 1 layer and 2 layers\n",
    "  - `dropout`: In the case of 2 layers, try several different dropouts, including 0.\n",
    "- Use early stopping with `patience = 50`;  \n",
    "You do not have to try every possible combination of these parameter choices; a good strategy is to\n",
    "try them separately, and then try a couple of combinations of the best choices of each. \n",
    "\n",
    "It is your choice about the other hyperparameters.  \n",
    "\n",
    "Provide a brief discussion of what you discovered, your best loss and accuracy measures for\n",
    "validation, and three versions of your testing accuracy, as in Part B.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide an analysis of what experiments you conducted with hyperparameters, what your results were, and in particular comment on how the two methods compare, especially given that one has *no* choice of hyperparameters, and one has *many* choices of parameters. How useful was the flexibility of choice in hyperparameters in Part C?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional:\n",
    "\n",
    "You might want to try doing this problem with a transformer model such as BERT. There are plenty of blog posts out there describing the details, and, as usual, chatGPT would have plenty of things to say about the topic.... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
